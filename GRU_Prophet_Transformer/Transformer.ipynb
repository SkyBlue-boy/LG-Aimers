{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd8aa66e",
   "metadata": {},
   "source": [
    "- batch size 현재 512 -> 1024로 진행 가능한지\n",
    "- 아래 코드 잘 실행되면 weekend, price 컬럼 추가해서 돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc487bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910f62dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# [Hyperparameter Setting]\n",
    "CFG = {\n",
    "    'TRAIN_WINDOW_SIZE': 90,  # 90일치로 학습\n",
    "    'PREDICT_SIZE': 21,  # 21일치 예측\n",
    "    'EPOCHS': 20,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'BATCH_SIZE': 512,\n",
    "    'SEED': 41,\n",
    "    'NUM_ENCODER_LAYERS': 2,  # Number of Transformer Encoder layers\n",
    "    'NUM_HEADS': 8,  # Number of attention heads in Transformer\n",
    "    'HIDDEN_SIZE': 512,  # Hidden size of Transformer model\n",
    "    'DROPOUT': 0.1,  # Dropout rate in Transformer\n",
    "}\n",
    "\n",
    "\n",
    "# [Import data]\n",
    "train_data = pd.read_csv('data/train.csv').drop(columns=['ID', '제품'])\n",
    "\n",
    "\n",
    "# Data Scaling\n",
    "# 숫자형 변수들의 min-max scaling을 수행하는 코드입니다.\n",
    "numeric_cols = train_data.columns[4:]\n",
    "# 칵 column의 min 및 max 계산\n",
    "min_values = train_data[numeric_cols].min(axis=1)\n",
    "max_values = train_data[numeric_cols].max(axis=1)\n",
    "# 각 행의 범위(max-min)를 계산하고, 범위가 0인 경우 1로 대체\n",
    "ranges = max_values - min_values\n",
    "ranges[ranges == 0] = 1\n",
    "# min-max scaling 수행\n",
    "train_data[numeric_cols] = (train_data[numeric_cols].subtract(min_values, axis=0)).div(ranges, axis=0)\n",
    "# max와 min 값을 dictionary 형태로 저장\n",
    "scale_min_dict = min_values.to_dict()\n",
    "scale_max_dict = max_values.to_dict()\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = ['대분류', '중분류', '소분류', '브랜드']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    label_encoder.fit(train_data[col])\n",
    "    train_data[col] = label_encoder.transform(train_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b549484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_data(data, train_size=CFG['TRAIN_WINDOW_SIZE'], predict_size=CFG['PREDICT_SIZE']):\n",
    "    '''\n",
    "    학습 기간 블럭, 예측 기간 블럭의 세트로 데이터를 생성\n",
    "    data : 일별 판매량\n",
    "    train_size : 학습에 활용할 기간\n",
    "    predict_size : 추론할 기간\n",
    "    '''\n",
    "    num_rows = len(data)\n",
    "    window_size = train_size + predict_size\n",
    "    \n",
    "    input_data = np.empty((num_rows * (len(data.columns) - window_size + 1), train_size, len(data.iloc[0, :4]) + 1))\n",
    "    target_data = np.empty((num_rows * (len(data.columns) - window_size + 1), predict_size))\n",
    "    \n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :4])\n",
    "        sales_data = np.array(data.iloc[i, 4:])\n",
    "        \n",
    "        for j in range(len(sales_data) - window_size + 1):\n",
    "            window = sales_data[j : j + window_size]\n",
    "            temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))\n",
    "            input_data[i * (len(data.columns) - window_size + 1) + j] = temp_data\n",
    "            target_data[i * (len(data.columns) - window_size + 1) + j] = window[train_size:]\n",
    "    \n",
    "    return input_data, target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37ee0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predict_data(data, train_size=CFG['TRAIN_WINDOW_SIZE']):\n",
    "    num_rows = len(data)\n",
    "    input_data = np.empty((num_rows, train_size, len(data.iloc[0, :4]) + 1))\n",
    "\n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :4])\n",
    "        sales_data = np.array(data.iloc[i, -train_size:])\n",
    "\n",
    "        window = sales_data[-train_size:]\n",
    "        temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))\n",
    "        input_data[i] = temp_data\n",
    "\n",
    "    return input_data\n",
    "\n",
    "train_input, train_target = make_train_data(train_data)\n",
    "test_input = make_predict_data(train_data)\n",
    "\n",
    "# Train / Validation Split\n",
    "data_len = len(train_input)\n",
    "val_input = train_input[-int(data_len * 0.2):]\n",
    "val_target = train_target[-int(data_len * 0.2):]\n",
    "train_input = train_input[:-int(data_len * 0.2)]\n",
    "train_target = train_target[:-int(data_len * 0.2)]\n",
    "\n",
    "# [Custom Dataset]\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.Y is not None:\n",
    "            return torch.Tensor(self.X[index]), torch.Tensor(self.Y[index])\n",
    "        return torch.Tensor(self.X[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "train_dataset = CustomDataset(train_input, train_target)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = CustomDataset(val_input, val_target)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6353c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Transformer Model]\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_size=CFG['HIDDEN_SIZE'], output_size=CFG['PREDICT_SIZE'],\n",
    "                 num_encoder_layers=CFG['NUM_ENCODER_LAYERS'], num_heads=CFG['NUM_HEADS'], dropout=CFG['DROPOUT']):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_size, nhead=num_heads, dropout=dropout),\n",
    "            num_encoder_layers)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_size // 2, output_size)\n",
    "        )\n",
    "        self.actv = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Shape: (B, TRAIN_WINDOW_SIZE, HIDDEN_SIZE)\n",
    "        x = x.permute(1, 0, 2)  # Shape: (TRAIN_WINDOW_SIZE, B, HIDDEN_SIZE)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        x = self.transformer_encoder(x)  # Shape: (TRAIN_WINDOW_SIZE, B, HIDDEN_SIZE)\n",
    "        x = x.permute(1, 0, 2)  # Shape: (B, TRAIN_WINDOW_SIZE, HIDDEN_SIZE)\n",
    "\n",
    "        # Fully connected layer\n",
    "        x = self.actv(self.fc(x[:, -1, :]))\n",
    "\n",
    "        return x\n",
    "\n",
    "# [Model Training]\n",
    "def train_transformer(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    best_loss = 9999999\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(1, CFG['EPOCHS'] + 1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "\n",
    "        for X, Y in tqdm(iter(train_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        val_loss = validation_transformer(model, val_loader, criterion, device)\n",
    "        print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : [{val_loss:.5f}]')\n",
    "\n",
    "        if best_loss > val_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "            print('Model Saved')\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def validation_transformer(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, Y in tqdm(iter(val_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(val_loss)\n",
    "\n",
    "# [Run !!]\n",
    "transformer_model = TransformerModel()\n",
    "optimizer = torch.optim.Adam(params=transformer_model.parameters(), lr=CFG[\"LEARNING_RATE\"])\n",
    "trained_transformer_model = train_transformer(transformer_model, optimizer, train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c785b8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Model Inference]\n",
    "test_dataset = CustomDataset(test_input, None)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X in tqdm(iter(test_loader)):\n",
    "            X = X.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "\n",
    "            # Move model output to CPU and convert to numpy array\n",
    "            output = output.cpu().numpy()\n",
    "\n",
    "            predictions.extend(output)\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "transformer_pred = inference(trained_transformer_model, test_loader, device)\n",
    "\n",
    "# [Inverse Scaling and Post-processing]\n",
    "for idx in range(len(transformer_pred)):\n",
    "    transformer_pred[idx, :] = transformer_pred[idx, :] * (scale_max_dict[idx] - scale_min_dict[idx]) + scale_min_dict[idx]\n",
    "\n",
    "# Post-processing\n",
    "transformer_pred = np.round(transformer_pred, 0).astype(int)\n",
    "\n",
    "# [Submission]\n",
    "submit = pd.read_csv('data/sample_submission.csv')\n",
    "submit.iloc[:, 1:] = transformer_pred\n",
    "submit.to_csv('data/transformer_submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dac08d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2ad35d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae782be9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
