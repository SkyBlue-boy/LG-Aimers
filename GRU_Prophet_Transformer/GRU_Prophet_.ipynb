{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77e39e9f",
   "metadata": {},
   "source": [
    "## RNN 기반 순환신경망 구조 -> Attention 메커니즘 함께 사용\n",
    "\n",
    "-> Luong Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ccf878",
   "metadata": {},
   "source": [
    "# Stacked RNN model (with LSTM layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b57af76c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-03T06:49:52.403184Z",
     "start_time": "2023-08-03T06:45:10.586339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411b1de8716d46ce9e5e08ff0e3632b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a42ee60783c425496e9b5d4a6aa4e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7743ef841b2f43b9b57600b248dffcfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# [Hyperparameter Setting]\n",
    "CFG = {\n",
    "    'TRAIN_WINDOW_SIZE': 90,  # 90일치로 학습\n",
    "    'PREDICT_SIZE': 21,  # 21일치 예측\n",
    "    'EPOCHS': 10,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'BATCH_SIZE': 4096,\n",
    "    'SEED': 41,\n",
    "    'STACKED_RNN_LAYERS': 2  # Number of stacked RNN layers\n",
    "}\n",
    "\n",
    "# [Import data]\n",
    "train_data = pd.read_csv('data/train.csv').drop(columns=['ID', '제품'])\n",
    "\n",
    "# [Data Preprocessing]\n",
    "scale_max_dict = {}\n",
    "scale_min_dict = {}\n",
    "\n",
    "for idx in tqdm(range(len(train_data))):\n",
    "    maxi = np.max(train_data.iloc[idx, 4:])\n",
    "    mini = np.min(train_data.iloc[idx, 4:])\n",
    "\n",
    "    if maxi == mini:\n",
    "        train_data.iloc[idx, 4:] = 0\n",
    "    else:\n",
    "        train_data.iloc[idx, 4:] = (train_data.iloc[idx, 4:] - mini) / (maxi - mini)\n",
    "\n",
    "    scale_max_dict[idx] = maxi\n",
    "    scale_min_dict[idx] = mini\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = ['대분류', '중분류', '소분류', '브랜드']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    label_encoder.fit(train_data[col])\n",
    "    train_data[col] = label_encoder.transform(train_data[col])\n",
    "\n",
    "def make_train_data(data, train_size=CFG['TRAIN_WINDOW_SIZE'], predict_size=CFG['PREDICT_SIZE']):\n",
    "    num_rows = len(data)\n",
    "    window_size = train_size + predict_size\n",
    "\n",
    "    input_data = np.empty((num_rows * (len(data.columns) - window_size + 1), train_size, len(data.iloc[0, :4]) + 1))\n",
    "    target_data = np.empty((num_rows * (len(data.columns) - window_size + 1), predict_size))\n",
    "\n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :4])\n",
    "        sales_data = np.array(data.iloc[i, 4:])\n",
    "\n",
    "        for j in range(len(sales_data) - window_size + 1):\n",
    "            window = sales_data[j: j + window_size]\n",
    "            temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))\n",
    "            input_data[i * (len(data.columns) - window_size + 1) + j] = temp_data\n",
    "            target_data[i * (len(data.columns) - window_size + 1) + j] = window[train_size:]\n",
    "\n",
    "    return input_data, target_data\n",
    "\n",
    "def make_predict_data(data, train_size=CFG['TRAIN_WINDOW_SIZE']):\n",
    "    num_rows = len(data)\n",
    "    input_data = np.empty((num_rows, train_size, len(data.iloc[0, :4]) + 1))\n",
    "\n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :4])\n",
    "        sales_data = np.array(data.iloc[i, -train_size:])\n",
    "\n",
    "        window = sales_data[-train_size:]\n",
    "        temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))\n",
    "        input_data[i] = temp_data\n",
    "\n",
    "    return input_data\n",
    "\n",
    "train_input, train_target = make_train_data(train_data)\n",
    "test_input = make_predict_data(train_data)\n",
    "\n",
    "# Train / Validation Split\n",
    "data_len = len(train_input)\n",
    "val_input = train_input[-int(data_len * 0.2):]\n",
    "val_target = train_target[-int(data_len * 0.2):]\n",
    "train_input = train_input[:-int(data_len * 0.2)]\n",
    "train_target = train_target[:-int(data_len * 0.2)]\n",
    "\n",
    "# [Custom Dataset]\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.Y is not None:\n",
    "            return torch.Tensor(self.X[index]), torch.Tensor(self.Y[index])\n",
    "        return torch.Tensor(self.X[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "train_dataset = CustomDataset(train_input, train_target)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = CustomDataset(val_input, val_target)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "# [Stacked RNN Model]\n",
    "class StackedRNNModel(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_size=512, output_size=CFG['PREDICT_SIZE'], num_layers=CFG['STACKED_RNN_LAYERS']):\n",
    "        super(StackedRNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_size // 2, output_size)\n",
    "        )\n",
    "        self.actv = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size, x.device)\n",
    "\n",
    "        # RNN layer\n",
    "        rnn_out, hidden = self.rnn(x, hidden)\n",
    "\n",
    "        # Only use the last output sequence\n",
    "        last_output = rnn_out[:, -1, :]\n",
    "\n",
    "        # Fully connected layer\n",
    "        output = self.actv(self.fc(last_output))\n",
    "\n",
    "        return output.squeeze(1)\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # Initialize hidden state\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "# [Model Training]\n",
    "def train_stacked_rnn(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    best_loss = 9999999\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(1, CFG['EPOCHS'] + 1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "\n",
    "        for X, Y in tqdm(iter(train_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        val_loss = validation_stacked_rnn(model, val_loader, criterion, device)\n",
    "        print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : [{val_loss:.5f}]')\n",
    "\n",
    "        if best_loss > val_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "            print('Model Saved')\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def validation_stacked_rnn(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, Y in tqdm(iter(val_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3018f57c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-03T06:31:26.600027Z",
     "start_time": "2023-08-03T06:26:28.353183Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f153a2720d3f454999bd47ca768f2c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d41aecaf77943f6ae1e38069c4f954e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4c414ed45542cfb12015c1f216179c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f3629c080f4fd69ade71147210889f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.65 GiB total capacity; 8.91 GiB already allocated; 339.81 MiB free; 9.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 202\u001b[0m\n\u001b[1;32m    200\u001b[0m stacked_rnn_model \u001b[38;5;241m=\u001b[39m StackedRNNModel()\n\u001b[1;32m    201\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(params\u001b[38;5;241m=\u001b[39mstacked_rnn_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mCFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLEARNING_RATE\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 202\u001b[0m trained_stacked_rnn_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_stacked_rnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstacked_rnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# [Model Inference]\u001b[39;00m\n\u001b[1;32m    205\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(test_input, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[4], line 165\u001b[0m, in \u001b[0;36mtrain_stacked_rnn\u001b[0;34m(model, optimizer, train_loader, val_loader, device)\u001b[0m\n\u001b[1;32m    161\u001b[0m Y \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    163\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 165\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, Y)\n\u001b[1;32m    168\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 134\u001b[0m, in \u001b[0;36mStackedRNNModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    131\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_hidden(batch_size, x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# RNN layer\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m rnn_out, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Only use the last output sequence\u001b[39;00m\n\u001b[1;32m    137\u001b[0m last_output \u001b[38;5;241m=\u001b[39m rnn_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:471\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRNN_TANH\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 471\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn_tanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    475\u001b[0m         result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mrnn_relu(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    476\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m    477\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.65 GiB total capacity; 8.91 GiB already allocated; 339.81 MiB free; 9.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "# [Run !!]\n",
    "stacked_rnn_model = StackedRNNModel()\n",
    "optimizer = torch.optim.Adam(params=stacked_rnn_model.parameters(), lr=CFG[\"LEARNING_RATE\"])\n",
    "trained_stacked_rnn_model = train_stacked_rnn(stacked_rnn_model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "# [Model Inference]\n",
    "test_dataset = CustomDataset(test_input, None)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X in tqdm(iter(test_loader)):\n",
    "            X = X.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "\n",
    "            # Move model output to CPU and convert to numpy array\n",
    "            output = output.cpu().numpy()\n",
    "\n",
    "            predictions.extend(output)\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "stacked_rnn_pred = inference(trained_stacked_rnn_model, test_loader, device)\n",
    "\n",
    "# [Inverse Scaling and Post-processing]\n",
    "for idx in range(len(stacked_rnn_pred)):\n",
    "    stacked_rnn_pred[idx, :] = stacked_rnn_pred[idx, :] * (scale_max_dict[idx] - scale_min_dict[idx]) + scale_min_dict[idx]\n",
    "\n",
    "# Post-processing\n",
    "stacked_rnn_pred = np.round(stacked_rnn_pred, 0).astype(int)\n",
    "\n",
    "# [Submission]\n",
    "submit = pd.read_csv('data/sample_submission.csv')\n",
    "submit.iloc[:, 1:] = stacked_rnn_pred\n",
    "submit.to_csv('data/stacked_rnn_submit.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c974f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98ac21ed",
   "metadata": {},
   "source": [
    "# Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd73b3c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T16:51:21.502286Z",
     "start_time": "2023-08-26T16:51:19.904006Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bee3b154",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T16:51:26.286153Z",
     "start_time": "2023-08-26T16:51:21.504078Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# [Hyperparameter Setting]\n",
    "CFG = {\n",
    "    'TRAIN_WINDOW_SIZE': 90,  # 90일치로 학습\n",
    "    'PREDICT_SIZE': 21,  # 21일치 예측\n",
    "    'EPOCHS': 20,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'BATCH_SIZE': 512,\n",
    "    'SEED': 41,\n",
    "    'NUM_ENCODER_LAYERS': 2,  # Number of Transformer Encoder layers\n",
    "    'NUM_HEADS': 8,  # Number of attention heads in Transformer\n",
    "    'HIDDEN_SIZE': 512,  # Hidden size of Transformer model\n",
    "    'DROPOUT': 0.1,  # Dropout rate in Transformer\n",
    "}\n",
    "\n",
    "\n",
    "# [Import data]\n",
    "train_data = pd.read_csv('data/train.csv').drop(columns=['ID', '제품'])\n",
    "\n",
    "\n",
    "# Data Scaling\n",
    "# 숫자형 변수들의 min-max scaling을 수행하는 코드입니다.\n",
    "numeric_cols = train_data.columns[4:]\n",
    "# 칵 column의 min 및 max 계산\n",
    "min_values = train_data[numeric_cols].min(axis=1)\n",
    "max_values = train_data[numeric_cols].max(axis=1)\n",
    "# 각 행의 범위(max-min)를 계산하고, 범위가 0인 경우 1로 대체\n",
    "ranges = max_values - min_values\n",
    "ranges[ranges == 0] = 1\n",
    "# min-max scaling 수행\n",
    "train_data[numeric_cols] = (train_data[numeric_cols].subtract(min_values, axis=0)).div(ranges, axis=0)\n",
    "# max와 min 값을 dictionary 형태로 저장\n",
    "scale_min_dict = min_values.to_dict()\n",
    "scale_max_dict = max_values.to_dict()\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = ['대분류', '중분류', '소분류', '브랜드']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    label_encoder.fit(train_data[col])\n",
    "    train_data[col] = label_encoder.transform(train_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "847cb412",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T16:51:26.293594Z",
     "start_time": "2023-08-26T16:51:26.288206Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_train_data(data, train_size=CFG['TRAIN_WINDOW_SIZE'], predict_size=CFG['PREDICT_SIZE']):\n",
    "    '''\n",
    "    학습 기간 블럭, 예측 기간 블럭의 세트로 데이터를 생성\n",
    "    data : 일별 판매량\n",
    "    train_size : 학습에 활용할 기간\n",
    "    predict_size : 추론할 기간\n",
    "    '''\n",
    "    num_rows = len(data)\n",
    "    window_size = train_size + predict_size\n",
    "    \n",
    "    input_data = np.empty((num_rows * (len(data.columns) - window_size + 1), train_size, len(data.iloc[0, :4]) + 1))\n",
    "    target_data = np.empty((num_rows * (len(data.columns) - window_size + 1), predict_size))\n",
    "    \n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :4])\n",
    "        sales_data = np.array(data.iloc[i, 4:])\n",
    "        \n",
    "        for j in range(len(sales_data) - window_size + 1):\n",
    "            window = sales_data[j : j + window_size]\n",
    "            temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))\n",
    "            input_data[i * (len(data.columns) - window_size + 1) + j] = temp_data\n",
    "            target_data[i * (len(data.columns) - window_size + 1) + j] = window[train_size:]\n",
    "    \n",
    "    return input_data, target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5023b73d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T03:45:46.396710Z",
     "start_time": "2023-08-11T03:45:46.385461Z"
    }
   },
   "outputs": [],
   "source": [
    "# def make_train_data(data, train_size=CFG['TRAIN_WINDOW_SIZE'], predict_size=CFG['PREDICT_SIZE']):\n",
    "#     STEP_SIZE = 2 # 이 값을 본인의 환경에 맞게 조정\n",
    "    \n",
    "#     num_rows = len(data)\n",
    "#     window_size = train_size + predict_size\n",
    "#     adjusted_size = (len(data.columns) - window_size + 1) // STEP_SIZE\n",
    "\n",
    "#     input_data = np.empty((num_rows * adjusted_size, train_size, len(data.iloc[0, :4]) + 1))\n",
    "#     target_data = np.empty((num_rows * adjusted_size, predict_size))\n",
    "\n",
    "#     for i in tqdm(range(num_rows)):\n",
    "#         encode_info = np.array(data.iloc[i, :4])\n",
    "#         sales_data = np.array(data.iloc[i, 4:])\n",
    "\n",
    "#         for j in range(0, len(sales_data) - window_size + 1, STEP_SIZE):\n",
    "#             window = sales_data[j: j + window_size]\n",
    "#             temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))\n",
    "#             input_data[i * adjusted_size + j // STEP_SIZE] = temp_data\n",
    "#             target_data[i * adjusted_size + j // STEP_SIZE] = window[train_size:]\n",
    "\n",
    "#     return input_data, target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5232eef2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T16:52:46.967757Z",
     "start_time": "2023-08-26T16:51:26.295118Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e700567d364b5aa87f0db5eae78bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ad7888f8074045a4f90a79f6a2affa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_predict_data(data, train_size=CFG['TRAIN_WINDOW_SIZE']):\n",
    "    num_rows = len(data)\n",
    "    input_data = np.empty((num_rows, train_size, len(data.iloc[0, :4]) + 1))\n",
    "\n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :4])\n",
    "        sales_data = np.array(data.iloc[i, -train_size:])\n",
    "\n",
    "        window = sales_data[-train_size:]\n",
    "        temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))\n",
    "        input_data[i] = temp_data\n",
    "\n",
    "    return input_data\n",
    "\n",
    "train_input, train_target = make_train_data(train_data)\n",
    "test_input = make_predict_data(train_data)\n",
    "\n",
    "# Train / Validation Split\n",
    "data_len = len(train_input)\n",
    "val_input = train_input[-int(data_len * 0.2):]\n",
    "val_target = train_target[-int(data_len * 0.2):]\n",
    "train_input = train_input[:-int(data_len * 0.2)]\n",
    "train_target = train_target[:-int(data_len * 0.2)]\n",
    "\n",
    "# [Custom Dataset]\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.Y is not None:\n",
    "            return torch.Tensor(self.X[index]), torch.Tensor(self.Y[index])\n",
    "        return torch.Tensor(self.X[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "train_dataset = CustomDataset(train_input, train_target)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = CustomDataset(val_input, val_target)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "340fba4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T16:54:51.268960Z",
     "start_time": "2023-08-26T16:52:46.969714Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2696b4d20acd44fb97ad72510628d9f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8765 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 85\u001b[0m\n\u001b[1;32m     83\u001b[0m transformer_model \u001b[38;5;241m=\u001b[39m TransformerModel()\n\u001b[1;32m     84\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(params\u001b[38;5;241m=\u001b[39mtransformer_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mCFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLEARNING_RATE\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 85\u001b[0m trained_transformer_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 54\u001b[0m, in \u001b[0;36mtrain_transformer\u001b[0;34m(model, optimizer, train_loader, val_loader, device)\u001b[0m\n\u001b[1;32m     51\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     52\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 54\u001b[0m     train_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     56\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m validation_transformer(model, val_loader, criterion, device)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch : [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Train Loss : [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(train_loss)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Val Loss : [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# [Transformer Model]\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_size=CFG['HIDDEN_SIZE'], output_size=CFG['PREDICT_SIZE'],\n",
    "                 num_encoder_layers=CFG['NUM_ENCODER_LAYERS'], num_heads=CFG['NUM_HEADS'], dropout=CFG['DROPOUT']):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_size, nhead=num_heads, dropout=dropout),\n",
    "            num_encoder_layers)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_size // 2, output_size)\n",
    "        )\n",
    "        self.actv = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Shape: (B, TRAIN_WINDOW_SIZE, HIDDEN_SIZE)\n",
    "        x = x.permute(1, 0, 2)  # Shape: (TRAIN_WINDOW_SIZE, B, HIDDEN_SIZE)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        x = self.transformer_encoder(x)  # Shape: (TRAIN_WINDOW_SIZE, B, HIDDEN_SIZE)\n",
    "        x = x.permute(1, 0, 2)  # Shape: (B, TRAIN_WINDOW_SIZE, HIDDEN_SIZE)\n",
    "\n",
    "        # Fully connected layer\n",
    "        x = self.actv(self.fc(x[:, -1, :]))\n",
    "\n",
    "        return x\n",
    "\n",
    "# [Model Training]\n",
    "def train_transformer(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    best_loss = 9999999\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(1, CFG['EPOCHS'] + 1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "\n",
    "        for X, Y in tqdm(iter(train_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        val_loss = validation_transformer(model, val_loader, criterion, device)\n",
    "        print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : [{val_loss:.5f}]')\n",
    "\n",
    "        if best_loss > val_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "            print('Model Saved')\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def validation_transformer(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, Y in tqdm(iter(val_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(val_loss)\n",
    "\n",
    "# [Run !!]\n",
    "transformer_model = TransformerModel()\n",
    "optimizer = torch.optim.Adam(params=transformer_model.parameters(), lr=CFG[\"LEARNING_RATE\"])\n",
    "trained_transformer_model = train_transformer(transformer_model, optimizer, train_loader, val_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9547fa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Model Inference]\n",
    "test_dataset = CustomDataset(test_input, None)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X in tqdm(iter(test_loader)):\n",
    "            X = X.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "\n",
    "            # Move model output to CPU and convert to numpy array\n",
    "            output = output.cpu().numpy()\n",
    "\n",
    "            predictions.extend(output)\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "transformer_pred = inference(trained_transformer_model, test_loader, device)\n",
    "\n",
    "# [Inverse Scaling and Post-processing]\n",
    "for idx in range(len(transformer_pred)):\n",
    "    transformer_pred[idx, :] = transformer_pred[idx, :] * (scale_max_dict[idx] - scale_min_dict[idx]) + scale_min_dict[idx]\n",
    "\n",
    "# Post-processing\n",
    "transformer_pred = np.round(transformer_pred, 0).astype(int)\n",
    "\n",
    "# [Submission]\n",
    "submit = pd.read_csv('data/sample_submission.csv')\n",
    "submit.iloc[:, 1:] = transformer_pred\n",
    "submit.to_csv('data/transformer_submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5affa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d2aec98",
   "metadata": {},
   "source": [
    "# Transformer with Luong Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# [Hyperparameter Setting]\n",
    "CFG = {\n",
    "    'TRAIN_WINDOW_SIZE': 90,  # 90일치로 학습\n",
    "    'PREDICT_SIZE': 21,  # 21일치 예측\n",
    "    'EPOCHS': 10,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'BATCH_SIZE': 4096,\n",
    "    'SEED': 41,\n",
    "    'HIDDEN_SIZE': 512,  # Hidden size of Transformer model\n",
    "    'NUM_LAYERS': 6,  # Number of Transformer encoder layers\n",
    "    'NUM_HEADS': 8,  # Number of attention heads\n",
    "}\n",
    "\n",
    "# [Import data]\n",
    "train_data = pd.read_csv('data/train.csv').drop(columns=['ID', '제품'])\n",
    "\n",
    "# [Data Preprocessing]\n",
    "scale_max_dict = {}\n",
    "scale_min_dict = {}\n",
    "\n",
    "for idx in tqdm(range(len(train_data))):\n",
    "    maxi = np.max(train_data.iloc[idx, 4:])\n",
    "    mini = np.min(train_data.iloc[idx, 4:])\n",
    "\n",
    "    if maxi == mini:\n",
    "        train_data.iloc[idx, 4:] = 0\n",
    "    else:\n",
    "        train_data.iloc[idx, 4:] = (train_data.iloc[idx, 4:] - mini) / (maxi - mini)\n",
    "\n",
    "    scale_max_dict[idx] = maxi\n",
    "    scale_min_dict[idx] = mini\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = ['대분류', '중분류', '소분류', '브랜드']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    label_encoder.fit(train_data[col])\n",
    "    train_data[col] = label_encoder.transform(train_data[col])\n",
    "\n",
    "def make_train_data(data, train_size=CFG['TRAIN_WINDOW_SIZE'], predict_size=CFG['PREDICT_SIZE']):\n",
    "    num_rows = len(data)\n",
    "    window_size = train_size + predict_size\n",
    "\n",
    "    input_data = np.empty((num_rows * (len(data.columns) - window_size + 1), train_size, len(data.iloc[0, :4]) + 1))\n",
    "    target_data = np.empty((num_rows * (len(data.columns) - window_size + 1), predict_size))\n",
    "\n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :4])\n",
    "        sales_data = np.array(data.iloc[i, 4:])\n",
    "\n",
    "        for j in range(len(sales_data) - window_size + 1):\n",
    "            window = sales_data[j: j + window_size]\n",
    "            temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))\n",
    "            input_data[i * (len(data.columns) - window_size + 1) + j] = temp_data\n",
    "            target_data[i * (len(data.columns) - window_size + 1) + j] = window[train_size:]\n",
    "\n",
    "    return input_data, target_data\n",
    "\n",
    "def make_predict_data(data, train_size=CFG['TRAIN_WINDOW_SIZE']):\n",
    "    num_rows = len(data)\n",
    "    input_data = np.empty((num_rows, train_size, len(data.iloc[0, :4]) + 1))\n",
    "\n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :4])\n",
    "        sales_data = np.array(data.iloc[i, -train_size:])\n",
    "\n",
    "        window = sales_data[-train_size:]\n",
    "        temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))\n",
    "        input_data[i] = temp_data\n",
    "\n",
    "    return input_data\n",
    "\n",
    "train_input, train_target = make_train_data(train_data)\n",
    "test_input = make_predict_data(train_data)\n",
    "\n",
    "# Train / Validation Split\n",
    "data_len = len(train_input)\n",
    "val_input = train_input[-int(data_len * 0.2):]\n",
    "val_target = train_target[-int(data_len * 0.2):]\n",
    "train_input = train_input[:-int(data_len * 0.2)]\n",
    "train_target = train_target[:-int(data_len * 0.2)]\n",
    "\n",
    "# [Custom Dataset]\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.Y is not None:\n",
    "            return torch.Tensor(self.X[index]), torch.Tensor(self.Y[index])\n",
    "        return torch.Tensor(self.X[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "train_dataset = CustomDataset(train_input, train_target)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = CustomDataset(val_input, val_target)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "# [Transformer Model with Luong Attention]\n",
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.score = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        # decoder_hidden: (batch_size, trg_len, hidden_size)\n",
    "        # encoder_outputs: (batch_size, src_len, hidden_size)\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1)  # (batch_size, 1, trg_len, hidden_size)\n",
    "        energy = torch.tanh(self.score(encoder_outputs) + decoder_hidden)  # (batch_size, src_len, hidden_size)\n",
    "        attention_weights = F.softmax(energy, dim=1)  # (batch_size, src_len, hidden_size)\n",
    "        context = torch.bmm(attention_weights.transpose(1, 2), encoder_outputs)  # (batch_size, hidden_size, trg_len)\n",
    "        return context\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attention = nn.MultiheadAttention(hidden_size, num_heads, dropout=0.1)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size * 4, hidden_size)\n",
    "        )\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src2, _ = self.self_attention(src, src, src, attn_mask=src_mask)\n",
    "        src = src + src2\n",
    "        src = self.layer_norm1(src)\n",
    "        src2 = self.feed_forward(src)\n",
    "        src = src + src2\n",
    "        src = self.layer_norm2(src)\n",
    "        return src\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_size=CFG['HIDDEN_SIZE'], num_layers=CFG['NUM_LAYERS'], num_heads=CFG['NUM_HEADS']):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(hidden_size, num_heads) for _ in range(num_layers)])\n",
    "        self.attention = LuongAttention(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size * 2, CFG['PREDICT_SIZE'])\n",
    "        self.actv = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.transpose(0, 1)  # (trg_len, batch_size, hidden_size)\n",
    "\n",
    "        src_mask = None\n",
    "        encoder_outputs = []\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "            encoder_outputs.append(x)\n",
    "\n",
    "        encoder_outputs = torch.stack(encoder_outputs)\n",
    "        context = self.attention(x.transpose(0, 1), encoder_outputs)\n",
    "        output = torch.cat((context, x[-1]), dim=1)\n",
    "        output = self.actv(self.fc(output))\n",
    "        return output.squeeze(1)\n",
    "\n",
    "# [Model Training]\n",
    "def train_transformer_with_attention(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    best_loss = 9999999\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(1, CFG['EPOCHS'] + 1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for X, Y in tqdm(iter(train_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        val_loss = validation_transformer_with_attention(model, val_loader, criterion, device)\n",
    "        print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : [{val_loss:.5f}]')\n",
    "\n",
    "        if best_loss > val_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "            print('Model Saved')\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def validation_transformer_with_attention(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, Y in tqdm(iter(val_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(val_loss)\n",
    "\n",
    "# [Run !!]\n",
    "transformer_with_attention_model = TransformerEncoder()\n",
    "optimizer = torch.optim.Adam(params=transformer_with_attention_model.parameters(), lr=CFG[\"LEARNING_RATE\"])\n",
    "trained_transformer_with_attention_model = train_transformer_with_attention(transformer_with_attention_model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "# [Model Inference]\n",
    "test_dataset = CustomDataset(test_input, None)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X in tqdm(iter(test_loader)):\n",
    "            X = X.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "\n",
    "            # Move model output to CPU and convert to numpy array\n",
    "            output = output.cpu().numpy()\n",
    "\n",
    "            predictions.extend(output)\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "transformer_with_attention_pred = inference(trained_transformer_with_attention_model, test_loader, device)\n",
    "\n",
    "# [Inverse Scaling and Post-processing]\n",
    "for idx in range(len(transformer_with_attention_pred)):\n",
    "    transformer_with_attention_pred[idx, :] = transformer_with_attention_pred[idx, :] * (scale_max_dict[idx] - scale_min_dict[idx]) + scale_min_dict[idx]\n",
    "\n",
    "# Post-processing\n",
    "transformer_with_attention_pred = np.round(transformer_with_attention_pred, 0).astype(int)\n",
    "\n",
    "# [Submission]\n",
    "submit = pd.read_csv('data/sample_submission.csv')\n",
    "submit.iloc[:, 1:] = transformer_with_attention_pred\n",
    "submit.to_csv('data/transformer_with_attention_submit.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eca0935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d8ebfd0",
   "metadata": {},
   "source": [
    "# Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37bc5d02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T03:45:20.218761Z",
     "start_time": "2023-08-14T03:45:17.659403Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: Cython in /home/yskim/.local/lib/python3.10/site-packages (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ebf9551",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T04:07:46.485217Z",
     "start_time": "2023-08-14T04:07:46.480812Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip uninstall python-holidays\n",
    "# !pip install python-holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81c48616",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T04:03:52.563830Z",
     "start_time": "2023-08-14T04:03:49.648685Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pystan in /home/yskim/.local/lib/python3.10/site-packages (3.7.0)\n",
      "Requirement already satisfied: setuptools in /data/program_files/anaconda3/lib/python3.10/site-packages (from pystan) (65.6.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19 in /data/program_files/anaconda3/lib/python3.10/site-packages (from pystan) (1.23.5)\n",
      "Requirement already satisfied: httpstan<4.11,>=4.10 in /home/yskim/.local/lib/python3.10/site-packages (from pystan) (4.10.1)\n",
      "Requirement already satisfied: aiohttp<4.0,>=3.6 in /home/yskim/.local/lib/python3.10/site-packages (from pystan) (3.8.5)\n",
      "Requirement already satisfied: clikit<0.7,>=0.6 in /home/yskim/.local/lib/python3.10/site-packages (from pystan) (0.6.2)\n",
      "Requirement already satisfied: pysimdjson<6.0.0,>=5.0.2 in /home/yskim/.local/lib/python3.10/site-packages (from pystan) (5.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/yskim/.local/lib/python3.10/site-packages (from aiohttp<4.0,>=3.6->pystan) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/yskim/.local/lib/python3.10/site-packages (from aiohttp<4.0,>=3.6->pystan) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/yskim/.local/lib/python3.10/site-packages (from aiohttp<4.0,>=3.6->pystan) (1.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /data/program_files/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0,>=3.6->pystan) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/yskim/.local/lib/python3.10/site-packages (from aiohttp<4.0,>=3.6->pystan) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/yskim/.local/lib/python3.10/site-packages (from aiohttp<4.0,>=3.6->pystan) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /data/program_files/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0,>=3.6->pystan) (22.1.0)\n",
      "Requirement already satisfied: pastel<0.3.0,>=0.2.0 in /home/yskim/.local/lib/python3.10/site-packages (from clikit<0.7,>=0.6->pystan) (0.2.1)\n",
      "Requirement already satisfied: crashtest<0.4.0,>=0.3.0 in /home/yskim/.local/lib/python3.10/site-packages (from clikit<0.7,>=0.6->pystan) (0.3.1)\n",
      "Requirement already satisfied: pylev<2.0,>=1.3 in /home/yskim/.local/lib/python3.10/site-packages (from clikit<0.7,>=0.6->pystan) (1.4.0)\n",
      "Requirement already satisfied: appdirs<2.0,>=1.4 in /data/program_files/anaconda3/lib/python3.10/site-packages (from httpstan<4.11,>=4.10->pystan) (1.4.4)\n",
      "Requirement already satisfied: webargs<9.0,>=8.0 in /home/yskim/.local/lib/python3.10/site-packages (from httpstan<4.11,>=4.10->pystan) (8.3.0)\n",
      "Requirement already satisfied: marshmallow<4.0,>=3.10 in /home/yskim/.local/lib/python3.10/site-packages (from httpstan<4.11,>=4.10->pystan) (3.20.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /data/program_files/anaconda3/lib/python3.10/site-packages (from marshmallow<4.0,>=3.10->httpstan<4.11,>=4.10->pystan) (22.0)\n",
      "Requirement already satisfied: idna>=2.0 in /data/program_files/anaconda3/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp<4.0,>=3.6->pystan) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pystan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd034ce2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T04:04:03.398244Z",
     "start_time": "2023-08-14T04:03:57.946552Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting fbprophet\n",
      "  Using cached fbprophet-0.7.1.tar.gz (64 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: Cython>=0.22 in /home/yskim/.local/lib/python3.10/site-packages (from fbprophet) (3.0.0)\n",
      "Collecting cmdstanpy==0.9.5\n",
      "  Using cached cmdstanpy-0.9.5-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: pystan>=2.14 in /home/yskim/.local/lib/python3.10/site-packages (from fbprophet) (3.7.0)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /data/program_files/anaconda3/lib/python3.10/site-packages (from fbprophet) (1.23.5)\n",
      "Requirement already satisfied: pandas>=1.0.4 in /data/program_files/anaconda3/lib/python3.10/site-packages (from fbprophet) (1.5.3)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in /data/program_files/anaconda3/lib/python3.10/site-packages (from fbprophet) (3.7.0)\n",
      "Requirement already satisfied: LunarCalendar>=0.0.9 in /home/yskim/.local/lib/python3.10/site-packages (from fbprophet) (0.0.9)\n",
      "Requirement already satisfied: convertdate>=2.1.2 in /home/yskim/.local/lib/python3.10/site-packages (from fbprophet) (2.4.0)\n",
      "Requirement already satisfied: holidays>=0.10.2 in /home/yskim/.local/lib/python3.10/site-packages (from fbprophet) (0.29)\n",
      "Requirement already satisfied: setuptools-git>=1.2 in /home/yskim/.local/lib/python3.10/site-packages (from fbprophet) (1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /data/program_files/anaconda3/lib/python3.10/site-packages (from fbprophet) (2.8.2)\n",
      "Requirement already satisfied: tqdm>=4.36.1 in /data/program_files/anaconda3/lib/python3.10/site-packages (from fbprophet) (4.64.1)\n",
      "Requirement already satisfied: pymeeus<=1,>=0.3.13 in /home/yskim/.local/lib/python3.10/site-packages (from convertdate>=2.1.2->fbprophet) (0.5.12)\n",
      "Requirement already satisfied: ephem>=3.7.5.3 in /home/yskim/.local/lib/python3.10/site-packages (from LunarCalendar>=0.0.9->fbprophet) (4.1.4)\n",
      "Requirement already satisfied: pytz in /data/program_files/anaconda3/lib/python3.10/site-packages (from LunarCalendar>=0.0.9->fbprophet) (2022.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /data/program_files/anaconda3/lib/python3.10/site-packages (from matplotlib>=2.0.0->fbprophet) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /data/program_files/anaconda3/lib/python3.10/site-packages (from matplotlib>=2.0.0->fbprophet) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /data/program_files/anaconda3/lib/python3.10/site-packages (from matplotlib>=2.0.0->fbprophet) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /data/program_files/anaconda3/lib/python3.10/site-packages (from matplotlib>=2.0.0->fbprophet) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /data/program_files/anaconda3/lib/python3.10/site-packages (from matplotlib>=2.0.0->fbprophet) (4.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /data/program_files/anaconda3/lib/python3.10/site-packages (from matplotlib>=2.0.0->fbprophet) (22.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /data/program_files/anaconda3/lib/python3.10/site-packages (from matplotlib>=2.0.0->fbprophet) (1.0.5)\n",
      "Requirement already satisfied: httpstan<4.11,>=4.10 in /home/yskim/.local/lib/python3.10/site-packages (from pystan>=2.14->fbprophet) (4.10.1)\n",
      "Requirement already satisfied: aiohttp<4.0,>=3.6 in /home/yskim/.local/lib/python3.10/site-packages (from pystan>=2.14->fbprophet) (3.8.5)\n",
      "Requirement already satisfied: clikit<0.7,>=0.6 in /home/yskim/.local/lib/python3.10/site-packages (from pystan>=2.14->fbprophet) (0.6.2)\n",
      "Requirement already satisfied: pysimdjson<6.0.0,>=5.0.2 in /home/yskim/.local/lib/python3.10/site-packages (from pystan>=2.14->fbprophet) (5.0.2)\n",
      "Requirement already satisfied: setuptools in /data/program_files/anaconda3/lib/python3.10/site-packages (from pystan>=2.14->fbprophet) (65.6.3)\n",
      "Requirement already satisfied: six>=1.5 in /data/program_files/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.0->fbprophet) (1.16.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/yskim/.local/lib/python3.10/site-packages (from aiohttp<4.0,>=3.6->pystan>=2.14->fbprophet) (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /data/program_files/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0,>=3.6->pystan>=2.14->fbprophet) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/yskim/.local/lib/python3.10/site-packages (from aiohttp<4.0,>=3.6->pystan>=2.14->fbprophet) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/yskim/.local/lib/python3.10/site-packages (from aiohttp<4.0,>=3.6->pystan>=2.14->fbprophet) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/yskim/.local/lib/python3.10/site-packages (from aiohttp<4.0,>=3.6->pystan>=2.14->fbprophet) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/yskim/.local/lib/python3.10/site-packages (from aiohttp<4.0,>=3.6->pystan>=2.14->fbprophet) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /data/program_files/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0,>=3.6->pystan>=2.14->fbprophet) (22.1.0)\n",
      "Requirement already satisfied: pastel<0.3.0,>=0.2.0 in /home/yskim/.local/lib/python3.10/site-packages (from clikit<0.7,>=0.6->pystan>=2.14->fbprophet) (0.2.1)\n",
      "Requirement already satisfied: pylev<2.0,>=1.3 in /home/yskim/.local/lib/python3.10/site-packages (from clikit<0.7,>=0.6->pystan>=2.14->fbprophet) (1.4.0)\n",
      "Requirement already satisfied: crashtest<0.4.0,>=0.3.0 in /home/yskim/.local/lib/python3.10/site-packages (from clikit<0.7,>=0.6->pystan>=2.14->fbprophet) (0.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0,>=3.10 in /home/yskim/.local/lib/python3.10/site-packages (from httpstan<4.11,>=4.10->pystan>=2.14->fbprophet) (3.20.1)\n",
      "Requirement already satisfied: appdirs<2.0,>=1.4 in /data/program_files/anaconda3/lib/python3.10/site-packages (from httpstan<4.11,>=4.10->pystan>=2.14->fbprophet) (1.4.4)\n",
      "Requirement already satisfied: webargs<9.0,>=8.0 in /home/yskim/.local/lib/python3.10/site-packages (from httpstan<4.11,>=4.10->pystan>=2.14->fbprophet) (8.3.0)\n",
      "Requirement already satisfied: idna>=2.0 in /data/program_files/anaconda3/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp<4.0,>=3.6->pystan>=2.14->fbprophet) (3.4)\n",
      "Building wheels for collected packages: fbprophet\n",
      "  Building wheel for fbprophet (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[57 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib\n",
      "  \u001b[31m   \u001b[0m creating build/lib/fbprophet\n",
      "  \u001b[31m   \u001b[0m creating build/lib/fbprophet/stan_model\n",
      "  \u001b[31m   \u001b[0m Note: NumExpr detected 64 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "  \u001b[31m   \u001b[0m NumExpr defaulting to 8 threads.\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-ap65drzd/fbprophet_ce63adabab744a03b635b93cb004d373/setup.py\", line 122, in <module>\n",
      "  \u001b[31m   \u001b[0m     setup(\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/__init__.py\", line 87, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 185, in setup\n",
      "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 201, in run_commands\n",
      "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 969, in run_commands\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/dist.py\", line 1208, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/wheel/bdist_wheel.py\", line 325, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(\"build\")\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/dist.py\", line 1208, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/command/build.py\", line 132, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd_name)\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/dist.py\", line 1208, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-ap65drzd/fbprophet_ce63adabab744a03b635b93cb004d373/setup.py\", line 48, in run\n",
      "  \u001b[31m   \u001b[0m     build_models(target_dir)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-ap65drzd/fbprophet_ce63adabab744a03b635b93cb004d373/setup.py\", line 36, in build_models\n",
      "  \u001b[31m   \u001b[0m     from fbprophet.models import StanBackendEnum\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-ap65drzd/fbprophet_ce63adabab744a03b635b93cb004d373/fbprophet/__init__.py\", line 8, in <module>\n",
      "  \u001b[31m   \u001b[0m     from fbprophet.forecaster import Prophet\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-ap65drzd/fbprophet_ce63adabab744a03b635b93cb004d373/fbprophet/forecaster.py\", line 17, in <module>\n",
      "  \u001b[31m   \u001b[0m     from fbprophet.make_holidays import get_holiday_names, make_holidays_df\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-ap65drzd/fbprophet_ce63adabab744a03b635b93cb004d373/fbprophet/make_holidays.py\", line 14, in <module>\n",
      "  \u001b[31m   \u001b[0m     import fbprophet.hdays as hdays_part2\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-ap65drzd/fbprophet_ce63adabab744a03b635b93cb004d373/fbprophet/hdays.py\", line 924, in <module>\n",
      "  \u001b[31m   \u001b[0m     class TU(Turkey):\n",
      "  \u001b[31m   \u001b[0m   File \"/home/yskim/.local/lib/python3.10/site-packages/holidays/registry.py\", line 172, in __init__\n",
      "  \u001b[31m   \u001b[0m     raise TypeError(\n",
      "  \u001b[31m   \u001b[0m TypeError: This is a python-holidays entity loader class. For entity inheritance purposes please import a class you want to derive from directly: e.g., `from holidays.countries import Entity` or `from holidays.financial import Entity`.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for fbprophet\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for fbprophet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to build fbprophet\n",
      "Installing collected packages: cmdstanpy, fbprophet\n",
      "  Attempting uninstall: cmdstanpy\n",
      "    Found existing installation: cmdstanpy 0.4.0\n",
      "    Uninstalling cmdstanpy-0.4.0:\n",
      "      Successfully uninstalled cmdstanpy-0.4.0\n",
      "  Running setup.py install for fbprophet ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mRunning setup.py install for fbprophet\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[61 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running install\n",
      "  \u001b[31m   \u001b[0m /data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib\n",
      "  \u001b[31m   \u001b[0m creating build/lib/fbprophet\n",
      "  \u001b[31m   \u001b[0m creating build/lib/fbprophet/stan_model\n",
      "  \u001b[31m   \u001b[0m Note: NumExpr detected 64 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "  \u001b[31m   \u001b[0m NumExpr defaulting to 8 threads.\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-ap65drzd/fbprophet_ce63adabab744a03b635b93cb004d373/setup.py\", line 122, in <module>\n",
      "  \u001b[31m   \u001b[0m     setup(\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/__init__.py\", line 87, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 185, in setup\n",
      "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 201, in run_commands\n",
      "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 969, in run_commands\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/dist.py\", line 1208, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/command/install.py\", line 68, in run\n",
      "  \u001b[31m   \u001b[0m     return orig.install.run(self)\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/command/install.py\", line 698, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command('build')\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/dist.py\", line 1208, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/command/build.py\", line 132, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd_name)\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/dist.py\", line 1208, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/data/program_files/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-ap65drzd/fbprophet_ce63adabab744a03b635b93cb004d373/setup.py\", line 48, in run\n",
      "  \u001b[31m   \u001b[0m     build_models(target_dir)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-ap65drzd/fbprophet_ce63adabab744a03b635b93cb004d373/setup.py\", line 36, in build_models\n",
      "  \u001b[31m   \u001b[0m     from fbprophet.models import StanBackendEnum\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-ap65drzd/fbprophet_ce63adabab744a03b635b93cb004d373/fbprophet/__init__.py\", line 8, in <module>\n",
      "  \u001b[31m   \u001b[0m     from fbprophet.forecaster import Prophet\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-ap65drzd/fbprophet_ce63adabab744a03b635b93cb004d373/fbprophet/forecaster.py\", line 17, in <module>\n",
      "  \u001b[31m   \u001b[0m     from fbprophet.make_holidays import get_holiday_names, make_holidays_df\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-ap65drzd/fbprophet_ce63adabab744a03b635b93cb004d373/fbprophet/make_holidays.py\", line 14, in <module>\n",
      "  \u001b[31m   \u001b[0m     import fbprophet.hdays as hdays_part2\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-ap65drzd/fbprophet_ce63adabab744a03b635b93cb004d373/fbprophet/hdays.py\", line 924, in <module>\n",
      "  \u001b[31m   \u001b[0m     class TU(Turkey):\n",
      "  \u001b[31m   \u001b[0m   File \"/home/yskim/.local/lib/python3.10/site-packages/holidays/registry.py\", line 172, in __init__\n",
      "  \u001b[31m   \u001b[0m     raise TypeError(\n",
      "  \u001b[31m   \u001b[0m TypeError: This is a python-holidays entity loader class. For entity inheritance purposes please import a class you want to derive from directly: e.g., `from holidays.countries import Entity` or `from holidays.financial import Entity`.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mlegacy-install-failure\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while trying to install package.\n",
      "\u001b[31m╰─>\u001b[0m fbprophet\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for output from the failure.\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install fbprophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54718b11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T03:51:46.307212Z",
     "start_time": "2023-08-14T03:51:45.295854Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fbprophet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfbprophet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Prophet\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# [Hyperparameter Setting]\u001b[39;00m\n\u001b[1;32m      8\u001b[0m CFG \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRAIN_WINDOW_SIZE\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m90\u001b[39m,  \u001b[38;5;66;03m# 90일치로 학습\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPREDICT_SIZE\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m21\u001b[39m,  \u001b[38;5;66;03m# 21일치 예측\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSEED\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m41\u001b[39m\n\u001b[1;32m     12\u001b[0m }\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fbprophet'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from fbprophet import Prophet\n",
    "\n",
    "# [Hyperparameter Setting]\n",
    "CFG = {\n",
    "    'TRAIN_WINDOW_SIZE': 90,  # 90일치로 학습\n",
    "    'PREDICT_SIZE': 21,  # 21일치 예측\n",
    "    'SEED': 41\n",
    "}\n",
    "\n",
    "# [Import data]\n",
    "train_data = pd.read_csv('data/train.csv').drop(columns=['ID', '제품'])\n",
    "\n",
    "# [Data Preprocessing]\n",
    "scale_max_dict = {}\n",
    "scale_min_dict = {}\n",
    "\n",
    "for idx in tqdm(range(len(train_data))):\n",
    "    maxi = np.max(train_data.iloc[idx, 4:])\n",
    "    mini = np.min(train_data.iloc[idx, 4:])\n",
    "    \n",
    "    if maxi == mini:\n",
    "        train_data.iloc[idx, 4:] = 0\n",
    "    else:\n",
    "        train_data.iloc[idx, 4:] = (train_data.iloc[idx, 4:] - mini) / (maxi - mini)\n",
    "    \n",
    "    scale_max_dict[idx] = maxi\n",
    "    scale_min_dict[idx] = mini\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = ['대분류', '중분류', '소분류', '브랜드']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    label_encoder.fit(train_data[col])\n",
    "    train_data[col] = label_encoder.transform(train_data[col])\n",
    "\n",
    "def make_train_data(data, train_size=CFG['TRAIN_WINDOW_SIZE'], predict_size=CFG['PREDICT_SIZE']):\n",
    "    '''\n",
    "    학습 기간 블럭, 예측 기간 블럭의 세트로 데이터를 생성\n",
    "    data : 일별 판매량\n",
    "    train_size : 학습에 활용할 기간\n",
    "    predict_size : 추론할 기간\n",
    "    '''\n",
    "    num_rows = len(data)\n",
    "    window_size = train_size + predict_size\n",
    "    \n",
    "    input_data = np.empty((num_rows * (len(data.columns) - window_size + 1), train_size, len(data.iloc[0, :4]) + 1))\n",
    "    target_data = np.empty((num_rows * (len(data.columns) - window_size + 1), predict_size))\n",
    "    \n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :4])\n",
    "        sales_data = np.array(data.iloc[i, 4:])\n",
    "        \n",
    "        for j in range(len(sales_data) - window_size + 1):\n",
    "            window = sales_data[j: j + window_size]\n",
    "            temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))\n",
    "            input_data[i * (len(data.columns) - window_size + 1) + j] = temp_data\n",
    "            target_data[i * (len(data.columns) - window_size + 1) + j] = window[train_size:]\n",
    "    \n",
    "    return input_data, target_data\n",
    "\n",
    "def make_predict_data(data, train_size=CFG['TRAIN_WINDOW_SIZE']):\n",
    "    '''\n",
    "    평가 데이터(Test Dataset)를 추론하기 위한 Input 데이터를 생성\n",
    "    data : 일별 판매량\n",
    "    train_size : 추론을 위해 필요한 일별 판매량 기간 (= 학습에 활용할 기간)\n",
    "    '''\n",
    "    num_rows = len(data)\n",
    "    \n",
    "    input_data = np.empty((num_rows, train_size, len(data.iloc[0, :4]) + 1))\n",
    "    \n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :4])\n",
    "        sales_data = np.array(data.iloc[i, -train_size:])\n",
    "        \n",
    "        window = sales_data[-train_size:]\n",
    "        temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))\n",
    "        input_data[i] = temp_data\n",
    "    \n",
    "    return input_data\n",
    "\n",
    "train_input, train_target = make_train_data(train_data)\n",
    "test_input = make_predict_data(train_data)\n",
    "\n",
    "# Since we're using Prophet, we don't need to split the data into train and validation sets.\n",
    "\n",
    "# [Prophet Model Training]\n",
    "def train_prophet(data):\n",
    "    data['ds'] = pd.date_range(start='2020-03-01', periods=len(data))\n",
    "    data['y'] = data.iloc[:, -CFG['PREDICT_SIZE']:].mean(axis=1)  # Use the mean of the target values for the prediction\n",
    "    \n",
    "    model = Prophet()\n",
    "    model.fit(data)\n",
    "    return model\n",
    "\n",
    "prophet_models = []\n",
    "for idx in tqdm(range(len(train_input))):\n",
    "    df = pd.DataFrame(data=train_input[idx], columns=['대분류', '중분류', '소분류', '브랜드', 'y'])\n",
    "    model = train_prophet(df)\n",
    "    prophet_models.append(model)\n",
    "\n",
    "# [Prophet Model Inference]\n",
    "def inference_prophet(model, data):\n",
    "    future = pd.DataFrame(data=data, columns=['대분류', '중분류', '소분류', '브랜드', 'y'])\n",
    "    forecast = model.predict(future)\n",
    "    return forecast['yhat'].values\n",
    "\n",
    "prophet_preds = []\n",
    "for idx in tqdm(range(len(test_input))):\n",
    "    model = prophet_models[idx]\n",
    "    forecast = inference_prophet(model, test_input[idx])\n",
    "    prophet_preds.append(forecast)\n",
    "\n",
    "# [Inverse Scaling and Post-processing]\n",
    "# (scaling에 대한 코드는 이전에 사용한 코드에서 가져온 것으로 가정)\n",
    "for idx in range(len(prophet_preds)):\n",
    "    prophet_preds[idx] = prophet_preds[idx] * (scale_max_dict[idx] - scale_min_dict[idx]) + scale_min_dict[idx]\n",
    "\n",
    "# 결과 후처리\n",
    "prophet_preds = np.round(prophet_preds, 0).astype(int)\n",
    "\n",
    "# [Submission]\n",
    "submit = pd.read_csv('data/sample_submission.csv')\n",
    "submit.iloc[:, 1:] = prophet_preds\n",
    "submit.to_csv('data/prophet_submit.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf9c59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9f9379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bc5739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a6bc7fb",
   "metadata": {},
   "source": [
    "# Stacked LSTM with Luong Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc0b040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# [Hyperparameter Setting]\n",
    "CFG = {\n",
    "    'TRAIN_WINDOW_SIZE': 90,  # 90일치로 학습\n",
    "    'PREDICT_SIZE': 21,  # 21일치 예측\n",
    "    'EPOCHS': 10,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'BATCH_SIZE': 4096,\n",
    "    'SEED': 41,\n",
    "    'HIDDEN_SIZE': 512,  # Hidden size of LSTM model\n",
    "    'NUM_LAYERS': 2,  # Number of stacked LSTM layers\n",
    "}\n",
    "\n",
    "# [Import data]\n",
    "train_data = pd.read_csv('data/train.csv').drop(columns=['ID', '제품'])\n",
    "\n",
    "# [Data Preprocessing]\n",
    "scale_max_dict = {}\n",
    "scale_min_dict = {}\n",
    "\n",
    "for idx in tqdm(range(len(train_data))):\n",
    "    maxi = np.max(train_data.iloc[idx, 4:])\n",
    "    mini = np.min(train_data.iloc[idx, 4:])\n",
    "\n",
    "    if maxi == mini:\n",
    "        train_data.iloc[idx, 4:] = 0\n",
    "    else:\n",
    "        train_data.iloc[idx, 4:] = (train_data.iloc[idx, 4:] - mini) / (maxi - mini)\n",
    "\n",
    "    scale_max_dict[idx] = maxi\n",
    "    scale_min_dict[idx] = mini\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = ['대분류', '중분류', '소분류', '브랜드']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    label_encoder.fit(train_data[col])\n",
    "    train_data[col] = label_encoder.transform(train_data[col])\n",
    "\n",
    "def make_train_data(data, train_size=CFG['TRAIN_WINDOW_SIZE'], predict_size=CFG['PREDICT_SIZE']):\n",
    "    num_rows = len(data)\n",
    "    window_size = train_size + predict_size\n",
    "\n",
    "    input_data = np.empty((num_rows * (len(data.columns) - window_size + 1), train_size, len(data.iloc[0, :4]) + 1))\n",
    "    target_data = np.empty((num_rows * (len(data.columns) - window_size + 1), predict_size))\n",
    "\n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :4])\n",
    "        sales_data = np.array(data.iloc[i, 4:])\n",
    "\n",
    "        for j in range(len(sales_data) - window_size + 1):\n",
    "            window = sales_data[j: j + window_size]\n",
    "            temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))\n",
    "            input_data[i * (len(data.columns) - window_size + 1) + j] = temp_data\n",
    "            target_data[i * (len(data.columns) - window_size + 1) + j] = window[train_size:]\n",
    "\n",
    "    return input_data, target_data\n",
    "\n",
    "def make_predict_data(data, train_size=CFG['TRAIN_WINDOW_SIZE']):\n",
    "    num_rows = len(data)\n",
    "    input_data = np.empty((num_rows, train_size, len(data.iloc[0, :4]) + 1))\n",
    "\n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :4])\n",
    "        sales_data = np.array(data.iloc[i, -train_size:])\n",
    "\n",
    "        window = sales_data[-train_size:]\n",
    "        temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))\n",
    "        input_data[i] = temp_data\n",
    "\n",
    "    return input_data\n",
    "\n",
    "train_input, train_target = make_train_data(train_data)\n",
    "test_input = make_predict_data(train_data)\n",
    "\n",
    "# Train / Validation Split\n",
    "data_len = len(train_input)\n",
    "val_input = train_input[-int(data_len * 0.2):]\n",
    "val_target = train_target[-int(data_len * 0.2):]\n",
    "train_input = train_input[:-int(data_len * 0.2)]\n",
    "train_target = train_target[:-int(data_len * 0.2)]\n",
    "\n",
    "# [Custom Dataset]\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.Y is not None:\n",
    "            return torch.Tensor(self.X[index]), torch.Tensor(self.Y[index])\n",
    "        return torch.Tensor(self.X[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "train_dataset = CustomDataset(train_input, train_target)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = CustomDataset(val_input, val_target)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "# [LSTM Model with Luong Attention]\n",
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.score = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        # decoder_hidden: (num_layers * num_directions, batch_size, hidden_size)\n",
    "        # encoder_outputs: (batch_size, seq_len, hidden_size)\n",
    "        decoder_hidden = decoder_hidden[-1]  # Select the last layer (num_layers * num_directions)\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1)  # (batch_size, 1, hidden_size)\n",
    "        energy = torch.tanh(self.score(encoder_outputs) + decoder_hidden)  # (batch_size, seq_len, hidden_size)\n",
    "        attention_weights = F.softmax(energy, dim=1)  # (batch_size, seq_len, hidden_size)\n",
    "        context = torch.bmm(attention_weights.transpose(1, 2), encoder_outputs)  # (batch_size, 1, hidden_size)\n",
    "        return context.squeeze(1)  # (batch_size, hidden_size)\n",
    "\n",
    "class StackedLSTMWithLuongAttention(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_size=CFG['HIDDEN_SIZE'], output_size=CFG['PREDICT_SIZE'], num_layers=CFG['NUM_LAYERS']):\n",
    "        super(StackedLSTMWithLuongAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.attention = LuongAttention(hidden_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_size // 2, output_size)\n",
    "        )\n",
    "        self.actv = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Shape: (B, TRAIN_WINDOW_SIZE, HIDDEN_SIZE)\n",
    "        output, (hidden, cell) = self.lstm(x)  # Shape: (B, TRAIN_WINDOW_SIZE, HIDDEN_SIZE)\n",
    "        context = self.attention(hidden, output)  # Shape: (B, HIDDEN_SIZE)\n",
    "        output = torch.cat((context, hidden[-1]), dim=1)  # Shape: (B, HIDDEN_SIZE * 2)\n",
    "        output = self.actv(self.fc(output))  # Shape: (B, PREDICT_SIZE)\n",
    "        return output\n",
    "\n",
    "# [Model Training]\n",
    "def train_stacked_lstm_with_attention(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    best_loss = 9999999\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(1, CFG['EPOCHS'] + 1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for X, Y in tqdm(iter(train_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        val_loss = validation_stacked_lstm_with_attention(model, val_loader, criterion, device)\n",
    "        print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : [{val_loss:.5f}]')\n",
    "\n",
    "        if best_loss > val_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "            print('Model Saved')\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def validation_stacked_lstm_with_attention(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, Y in tqdm(iter(val_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(val_loss)\n",
    "\n",
    "# [Run !!]\n",
    "stacked_lstm_with_attention_model = StackedLSTMWithLuongAttention()\n",
    "optimizer = torch.optim.Adam(params=stacked_lstm_with_attention_model.parameters(), lr=CFG[\"LEARNING_RATE\"])\n",
    "trained_stacked_lstm_with_attention_model = train_stacked_lstm_with_attention(stacked_lstm_with_attention_model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "# [Model Inference]\n",
    "test_dataset = CustomDataset(test_input, None)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X in tqdm(iter(test_loader)):\n",
    "            X = X.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "\n",
    "            # Move model output to CPU and convert to numpy array\n",
    "            output = output.cpu().numpy()\n",
    "\n",
    "            predictions.extend(output)\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "stacked_lstm_with_attention_pred = inference(trained_stacked_lstm_with_attention_model, test_loader, device)\n",
    "\n",
    "# [Inverse Scaling and Post-processing]\n",
    "for idx in range(len(stacked_lstm_with_attention_pred)):\n",
    "    stacked_lstm_with_attention_pred[idx, :] = stacked_lstm_with_attention_pred[idx, :] * (scale_max_dict[idx] - scale_min_dict[idx]) + scale_min_dict[idx]\n",
    "\n",
    "# Post-processing\n",
    "stacked_lstm_with_attention_pred = np.round(stacked_lstm_with_attention_pred, 0).astype(int)\n",
    "\n",
    "# [Submission]\n",
    "submit = pd.read_csv('data/sample_submission.csv')\n",
    "submit.iloc[:, 1:] = stacked_lstm_with_attention_pred\n",
    "submit.to_csv('data/stacked_lstm_with_attention_submit.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0db21ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83efd8cc",
   "metadata": {},
   "source": [
    "# Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d5983a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-03T10:18:34.577135Z",
     "start_time": "2023-08-03T10:15:18.692480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8aebac8bf6422da9f63557ccbb3128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# [Hyperparameter Setting]\n",
    "CFG = {\n",
    "    'TRAIN_WINDOW_SIZE': 90,  # 90일치로 학습\n",
    "    'PREDICT_SIZE': 21,  # 21일치 예측\n",
    "    'EPOCHS': 10,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'BATCH_SIZE': 1024,\n",
    "    'SEED': 41,\n",
    "    'STACKED_LSTM_LAYERS': 2  # Number of stacked LSTM layers\n",
    "}\n",
    "\n",
    "# [Import data]\n",
    "train_data = pd.read_csv('data/train.csv').drop(columns=['ID', '제품'])\n",
    "\n",
    "# [Data Preprocessing]\n",
    "scale_max_dict = {}\n",
    "scale_min_dict = {}\n",
    "\n",
    "for idx in tqdm(range(len(train_data))):\n",
    "    maxi = np.max(train_data.iloc[idx, 4:])\n",
    "    mini = np.min(train_data.iloc[idx, 4:])\n",
    "\n",
    "    if maxi == mini:\n",
    "        train_data.iloc[idx, 4:] = 0\n",
    "    else:\n",
    "        train_data.iloc[idx, 4:] = (train_data.iloc[idx, 4:] - mini) / (maxi - mini)\n",
    "\n",
    "    scale_max_dict[idx] = maxi\n",
    "    scale_min_dict[idx] = mini\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = ['대분류', '중분류', '소분류', '브랜드']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    label_encoder.fit(train_data[col])\n",
    "    train_data[col] = label_encoder.transform(train_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85b03f5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-03T10:20:16.020993Z",
     "start_time": "2023-08-03T10:18:34.580529Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9beaa3c39a644728d52d40949cfc69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee71af9abb5c42d9b1819165dfe46995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539aaf3f28f244cb9b53722f5852b6ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4383 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_EXECUTION_FAILED",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 153\u001b[0m\n\u001b[1;32m    151\u001b[0m stacked_lstm_model \u001b[38;5;241m=\u001b[39m StackedLSTMModel()\n\u001b[1;32m    152\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(params\u001b[38;5;241m=\u001b[39mstacked_lstm_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mCFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLEARNING_RATE\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 153\u001b[0m trained_stacked_lstm_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_stacked_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstacked_lstm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# [Model Inference]\u001b[39;00m\n\u001b[1;32m    156\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(test_input, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[20], line 116\u001b[0m, in \u001b[0;36mtrain_stacked_lstm\u001b[0;34m(model, optimizer, train_loader, val_loader, device)\u001b[0m\n\u001b[1;32m    112\u001b[0m Y \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    114\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 116\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, Y)\n\u001b[1;32m    119\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[20], line 84\u001b[0m, in \u001b[0;36mStackedLSTMModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     81\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_hidden(batch_size, x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# LSTM layer\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m lstm_out, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Only use the last output sequence\u001b[39;00m\n\u001b[1;32m     87\u001b[0m last_output \u001b[38;5;241m=\u001b[39m lstm_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:769\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    773\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def make_train_data(data, train_size=CFG['TRAIN_WINDOW_SIZE'], predict_size=CFG['PREDICT_SIZE']):\n",
    "    num_rows = len(data)\n",
    "    window_size = train_size + predict_size\n",
    "\n",
    "    input_data = np.empty((num_rows * (len(data.columns) - window_size + 1), train_size, len(data.iloc[0, :4]) + 1))\n",
    "    target_data = np.empty((num_rows * (len(data.columns) - window_size + 1), predict_size))\n",
    "\n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :4])\n",
    "        sales_data = np.array(data.iloc[i, 4:])\n",
    "\n",
    "        for j in range(len(sales_data) - window_size + 1):\n",
    "            window = sales_data[j: j + window_size]\n",
    "            temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))\n",
    "            input_data[i * (len(data.columns) - window_size + 1) + j] = temp_data\n",
    "            target_data[i * (len(data.columns) - window_size + 1) + j] = window[train_size:]\n",
    "\n",
    "    return input_data, target_data\n",
    "\n",
    "def make_predict_data(data, train_size=CFG['TRAIN_WINDOW_SIZE']):\n",
    "    num_rows = len(data)\n",
    "    input_data = np.empty((num_rows, train_size, len(data.iloc[0, :4]) + 1))\n",
    "\n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :4])\n",
    "        sales_data = np.array(data.iloc[i, -train_size:])\n",
    "\n",
    "        window = sales_data[-train_size:]\n",
    "        temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))\n",
    "        input_data[i] = temp_data\n",
    "\n",
    "    return input_data\n",
    "\n",
    "train_input, train_target = make_train_data(train_data)\n",
    "test_input = make_predict_data(train_data)\n",
    "\n",
    "# Train / Validation Split\n",
    "data_len = len(train_input)\n",
    "val_input = train_input[-int(data_len * 0.2):]\n",
    "val_target = train_target[-int(data_len * 0.2):]\n",
    "train_input = train_input[:-int(data_len * 0.2)]\n",
    "train_target = train_target[:-int(data_len * 0.2)]\n",
    "\n",
    "# [Custom Dataset]\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.Y is not None:\n",
    "            return torch.Tensor(self.X[index]), torch.Tensor(self.Y[index])\n",
    "        return torch.Tensor(self.X[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "train_dataset = CustomDataset(train_input, train_target)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = CustomDataset(val_input, val_target)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "# [Stacked LSTM Model]\n",
    "class StackedLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_size=512, output_size=CFG['PREDICT_SIZE'], num_layers=CFG['STACKED_LSTM_LAYERS']):\n",
    "        super(StackedLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_size // 2, output_size)\n",
    "        )\n",
    "        self.actv = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size, x.device)\n",
    "\n",
    "        # LSTM layer\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "\n",
    "        # Only use the last output sequence\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "\n",
    "        # Fully connected layer\n",
    "        output = self.actv(self.fc(last_output))\n",
    "\n",
    "        return output.squeeze(1)\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # Initialize hidden state and cell state\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device))\n",
    "\n",
    "# [Model Training]\n",
    "def train_stacked_lstm(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    best_loss = 9999999\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(1, CFG['EPOCHS'] + 1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "\n",
    "        for X, Y in tqdm(iter(train_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        val_loss = validation_stacked_lstm(model, val_loader, criterion, device)\n",
    "        print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : [{val_loss:.5f}]')\n",
    "\n",
    "        if best_loss > val_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "            print('Model Saved')\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def validation_stacked_lstm(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, Y in tqdm(iter(val_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(val_loss)\n",
    "\n",
    "# [Run !!]\n",
    "stacked_lstm_model = StackedLSTMModel()\n",
    "optimizer = torch.optim.Adam(params=stacked_lstm_model.parameters(), lr=CFG[\"LEARNING_RATE\"])\n",
    "trained_stacked_lstm_model = train_stacked_lstm(stacked_lstm_model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "# [Model Inference]\n",
    "test_dataset = CustomDataset(test_input, None)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X in tqdm(iter(test_loader)):\n",
    "            X = X.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "\n",
    "            # Move model output to CPU and convert to numpy array\n",
    "            output = output.cpu().numpy()\n",
    "\n",
    "            predictions.extend(output)\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "stacked_lstm_pred = inference(trained_stacked_lstm_model, test_loader, device)\n",
    "\n",
    "# [Inverse Scaling and Post-processing]\n",
    "for idx in range(len(stacked_lstm_pred)):\n",
    "    stacked_lstm_pred[idx, :] = stacked_lstm_pred[idx, :] * (scale_max_dict[idx] - scale_min_dict[idx]) + scale_min_dict[idx]\n",
    "\n",
    "# Post-processing\n",
    "stacked_lstm_pred = np.round(stacked_lstm_pred, 0).astype(int)\n",
    "\n",
    "# [Submission]\n",
    "submit = pd.read_csv('data/sample_submission.csv')\n",
    "submit.iloc[:, 1:] = stacked_lstm_pred\n",
    "submit.to_csv('data/stacked_lstm_submit.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04af6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e5ba5c8",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5498b49e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-03T07:59:24.551431Z",
     "start_time": "2023-08-03T07:56:02.941003Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e3243ed39548978238dc70aa72b273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# [Hyperparameter Setting]\n",
    "CFG = {\n",
    "    'TRAIN_WINDOW_SIZE': 90,  # 90일치로 학습\n",
    "    'PREDICT_SIZE': 21,  # 21일치 예측\n",
    "    'EPOCHS': 10,            ########### 원래는 epoch 10임! #################\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'BATCH_SIZE': 1024,\n",
    "    'SEED': 41\n",
    "}\n",
    "\n",
    "# [Import data]\n",
    "train_data = pd.read_csv('data/train.csv').drop(columns=['ID', '제품'])\n",
    "\n",
    "# [Data Preprocessing]\n",
    "scale_max_dict = {}\n",
    "scale_min_dict = {}\n",
    "\n",
    "for idx in tqdm(range(len(train_data))):\n",
    "    maxi = np.max(train_data.iloc[idx, 4:])\n",
    "    mini = np.min(train_data.iloc[idx, 4:])\n",
    "\n",
    "    if maxi == mini:\n",
    "        train_data.iloc[idx, 4:] = 0\n",
    "    else:\n",
    "        train_data.iloc[idx, 4:] = (train_data.iloc[idx, 4:] - mini) / (maxi - mini)\n",
    "\n",
    "    scale_max_dict[idx] = maxi\n",
    "    scale_min_dict[idx] = mini\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = ['대분류', '중분류', '소분류', '브랜드']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    label_encoder.fit(train_data[col])\n",
    "    train_data[col] = label_encoder.transform(train_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63889ae7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-03T09:54:27.633172Z",
     "start_time": "2023-08-03T07:59:24.553754Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e085902e304ecb9b0d484b02a8e379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c7a2fb44bc43fcbe2f36435fa612c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28d88919239445e951958146cb1dab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4383 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551be21d29f3424d8801d0c2b2a31649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [1] Train Loss : [0.02687] Val Loss : [0.01863]\n",
      "Model Saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0321f243b448bd8320060ef734bc2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4383 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc39d775877455fbb66af83e0c6475e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [2] Train Loss : [0.01881] Val Loss : [0.01928]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f5d3e00ba64528b8e82e1ae6dbab39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4383 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5aa8877688e4faabf86704d85a8dea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [3] Train Loss : [0.01845] Val Loss : [0.01783]\n",
      "Model Saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3b4144eeba4c228dd65ef732c02310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4383 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eabf989a1990407ab363e32a5cc87b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [4] Train Loss : [0.01822] Val Loss : [0.02070]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46072babf52842288b967ed01a1d5141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4383 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62ddb9144dd4131b48e6897b1d2cdda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [5] Train Loss : [0.01801] Val Loss : [0.01756]\n",
      "Model Saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96592f15af3e4c7ca1495a7b8652e064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4383 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e06e773d88544f493261c39a08b16af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [6] Train Loss : [0.01794] Val Loss : [0.01800]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad14f916b65401ab1709c1155db3d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4383 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d735fcfecb44f7a3b6c9294d3ca35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [7] Train Loss : [0.01774] Val Loss : [0.01743]\n",
      "Model Saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f51928860a4d2497e64d47b364ef2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4383 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e04c900a10464db96fc496b5bddbd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [8] Train Loss : [0.01763] Val Loss : [0.01754]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f797714c24334723b0bb6d8b8ca0655c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4383 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c5194a3b1a4e2da08cfcf1b5607af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [9] Train Loss : [0.01759] Val Loss : [0.01882]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9feacb5b706d47948bb3ea60614242ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4383 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae2db4df8fe45e8b6fd950e9af3834e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [10] Train Loss : [0.01757] Val Loss : [0.01727]\n",
      "Model Saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a03064995b44478ae5ae4f65acc6d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_train_data(data, train_size=CFG['TRAIN_WINDOW_SIZE'], predict_size=CFG['PREDICT_SIZE']):\n",
    "    num_rows = len(data)\n",
    "    window_size = train_size + predict_size\n",
    "\n",
    "    input_data = np.empty((num_rows * (len(data.columns) - window_size + 1), train_size, len(data.iloc[0, :4]) + 1))\n",
    "    target_data = np.empty((num_rows * (len(data.columns) - window_size + 1), predict_size))\n",
    "\n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :4])\n",
    "        sales_data = np.array(data.iloc[i, 4:])\n",
    "\n",
    "        for j in range(len(sales_data) - window_size + 1):\n",
    "            window = sales_data[j: j + window_size]\n",
    "            temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))\n",
    "            input_data[i * (len(data.columns) - window_size + 1) + j] = temp_data\n",
    "            target_data[i * (len(data.columns) - window_size + 1) + j] = window[train_size:]\n",
    "\n",
    "    return input_data, target_data\n",
    "\n",
    "def make_predict_data(data, train_size=CFG['TRAIN_WINDOW_SIZE']):\n",
    "    num_rows = len(data)\n",
    "    input_data = np.empty((num_rows, train_size, len(data.iloc[0, :4]) + 1))\n",
    "\n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :4])\n",
    "        sales_data = np.array(data.iloc[i, -train_size:])\n",
    "\n",
    "        window = sales_data[-train_size:]\n",
    "        temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))\n",
    "        input_data[i] = temp_data\n",
    "\n",
    "    return input_data\n",
    "\n",
    "train_input, train_target = make_train_data(train_data)\n",
    "test_input = make_predict_data(train_data)\n",
    "\n",
    "# Train / Validation Split\n",
    "data_len = len(train_input)\n",
    "val_input = train_input[-int(data_len * 0.2):]\n",
    "val_target = train_target[-int(data_len * 0.2):]\n",
    "train_input = train_input[:-int(data_len * 0.2)]\n",
    "train_target = train_target[:-int(data_len * 0.2)]\n",
    "\n",
    "# [Custom Dataset]\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.Y is not None:\n",
    "            return torch.Tensor(self.X[index]), torch.Tensor(self.Y[index])\n",
    "        return torch.Tensor(self.X[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "train_dataset = CustomDataset(train_input, train_target)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = CustomDataset(val_input, val_target)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "# [GRU Model]\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_size=512, output_size=CFG['PREDICT_SIZE']):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_size // 2, output_size)\n",
    "        )\n",
    "        self.actv = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, TRAIN_WINDOW_SIZE, 5)\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size, x.device)\n",
    "\n",
    "        # GRU layer\n",
    "        gru_out, hidden = self.gru(x, hidden)\n",
    "\n",
    "        # Only use the last output sequence\n",
    "        last_output = gru_out[:, -1, :]\n",
    "\n",
    "        # Fully connected layer\n",
    "        output = self.actv(self.fc(last_output))\n",
    "\n",
    "        return output.squeeze(1)\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # Initialize hidden state\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "# [Model Training]\n",
    "def train_gru(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    best_loss = 9999999\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(1, CFG['EPOCHS'] + 1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "\n",
    "        for X, Y in tqdm(iter(train_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        val_loss = validation(model, val_loader, criterion, device)\n",
    "        print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : [{val_loss:.5f}]')\n",
    "\n",
    "        if best_loss > val_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "            print('Model Saved')\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def validation(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, Y in tqdm(iter(val_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(val_loss)\n",
    "\n",
    "# [Run !!]\n",
    "gru_model = GRUModel()\n",
    "optimizer = torch.optim.Adam(params=gru_model.parameters(), lr=CFG[\"LEARNING_RATE\"])\n",
    "trained_gru_model = train_gru(gru_model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "# [Model Inference]\n",
    "test_dataset = CustomDataset(test_input, None)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X in tqdm(iter(test_loader)):\n",
    "            X = X.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "\n",
    "            # Move model output to CPU and convert to numpy array\n",
    "            output = output.cpu().numpy()\n",
    "\n",
    "            predictions.extend(output)\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "gru_pred = inference(trained_gru_model, test_loader, device)\n",
    "\n",
    "# [Inverse Scaling and Post-processing]\n",
    "for idx in range(len(gru_pred)):\n",
    "    gru_pred[idx, :] = gru_pred[idx, :] * (scale_max_dict[idx] - scale_min_dict[idx]) + scale_min_dict[idx]\n",
    "\n",
    "# Post-processing\n",
    "gru_pred = np.round(gru_pred, 0).astype(int)\n",
    "\n",
    "# [Submission]\n",
    "submit = pd.read_csv('data/sample_submission.csv')\n",
    "submit.iloc[:, 1:] = gru_pred\n",
    "submit.to_csv('data/gru_submit.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385cc3c5",
   "metadata": {},
   "source": [
    "# GRU with Luong Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7841d314",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-04T02:20:52.542736Z",
     "start_time": "2023-08-04T02:17:25.641585Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bbc0c8c956f4e009b91d50c68432678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# [Hyperparameter Setting]\n",
    "CFG = {\n",
    "    'TRAIN_WINDOW_SIZE': 90,  # 90일치로 학습\n",
    "    'PREDICT_SIZE': 21,  # 21일치 예측\n",
    "    'EPOCHS': 10,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'BATCH_SIZE': 1024,\n",
    "    'SEED': 41,\n",
    "    'HIDDEN_SIZE': 512,  # Hidden size of GRU model\n",
    "}\n",
    "\n",
    "# [Import data]\n",
    "train_data = pd.read_csv('data/train.csv').drop(columns=['ID', '제품'])\n",
    "\n",
    "# [Data Preprocessing]\n",
    "scale_max_dict = {}\n",
    "scale_min_dict = {}\n",
    "\n",
    "for idx in tqdm(range(len(train_data))):\n",
    "    maxi = np.max(train_data.iloc[idx, 4:])\n",
    "    mini = np.min(train_data.iloc[idx, 4:])\n",
    "\n",
    "    if maxi == mini:\n",
    "        train_data.iloc[idx, 4:] = 0\n",
    "    else:\n",
    "        train_data.iloc[idx, 4:] = (train_data.iloc[idx, 4:] - mini) / (maxi - mini)\n",
    "\n",
    "    scale_max_dict[idx] = maxi\n",
    "    scale_min_dict[idx] = mini\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = ['대분류', '중분류', '소분류', '브랜드']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    label_encoder.fit(train_data[col])\n",
    "    train_data[col] = label_encoder.transform(train_data[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ab328ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-04T02:36:59.291520Z",
     "start_time": "2023-08-04T02:35:37.500270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2a33f6733e48c08cf2cf7504f4f008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4673dc8508e4f9ba8f75d5b5a67f4a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def make_train_data(data, train_size=CFG['TRAIN_WINDOW_SIZE'], predict_size=CFG['PREDICT_SIZE']):\n",
    "    num_rows = len(data)\n",
    "    window_size = train_size + predict_size\n",
    "\n",
    "    input_data = np.empty((num_rows * (len(data.columns) - window_size + 1), train_size, len(data.iloc[0, :4]) + 1))\n",
    "    target_data = np.empty((num_rows * (len(data.columns) - window_size + 1), predict_size))\n",
    "\n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :4])\n",
    "        sales_data = np.array(data.iloc[i, 4:])\n",
    "\n",
    "        for j in range(len(sales_data) - window_size + 1):\n",
    "            window = sales_data[j: j + window_size]\n",
    "            temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))\n",
    "            input_data[i * (len(data.columns) - window_size + 1) + j] = temp_data\n",
    "            target_data[i * (len(data.columns) - window_size + 1) + j] = window[train_size:]\n",
    "\n",
    "    return input_data, target_data\n",
    "\n",
    "def make_predict_data(data, train_size=CFG['TRAIN_WINDOW_SIZE']):\n",
    "    num_rows = len(data)\n",
    "    input_data = np.empty((num_rows, train_size, len(data.iloc[0, :4]) + 1))\n",
    "\n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :4])\n",
    "        sales_data = np.array(data.iloc[i, -train_size:])\n",
    "\n",
    "        window = sales_data[-train_size:]\n",
    "        temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))\n",
    "        input_data[i] = temp_data\n",
    "\n",
    "    return input_data\n",
    "\n",
    "train_input, train_target = make_train_data(train_data)\n",
    "test_input = make_predict_data(train_data)\n",
    "\n",
    "# Train / Validation Split\n",
    "data_len = len(train_input)\n",
    "val_input = train_input[-int(data_len * 0.2):]\n",
    "val_target = train_target[-int(data_len * 0.2):]\n",
    "train_input = train_input[:-int(data_len * 0.2)]\n",
    "train_target = train_target[:-int(data_len * 0.2)]\n",
    "\n",
    "# [Custom Dataset]\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.Y is not None:\n",
    "            return torch.Tensor(self.X[index]), torch.Tensor(self.Y[index])\n",
    "        return torch.Tensor(self.X[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "train_dataset = CustomDataset(train_input, train_target)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = CustomDataset(val_input, val_target)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "143b744f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-04T02:49:28.232489Z",
     "start_time": "2023-08-04T02:49:28.154492Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'HIDDEN_SIZE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m         context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(attention_weights\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m), encoder_outputs)  \u001b[38;5;66;03m# (batch_size, hidden_size, 1)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# (batch_size, hidden_size)\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGRUWithLuongAttention\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39mCFG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHIDDEN_SIZE\u001b[39m\u001b[38;5;124m'\u001b[39m], output_size\u001b[38;5;241m=\u001b[39mCFG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPREDICT_SIZE\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28msuper\u001b[39m(GRUWithLuongAttention, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m, in \u001b[0;36mGRUWithLuongAttention\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGRUWithLuongAttention\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[43mCFG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHIDDEN_SIZE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, output_size\u001b[38;5;241m=\u001b[39mCFG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPREDICT_SIZE\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28msuper\u001b[39m(GRUWithLuongAttention, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m=\u001b[39m hidden_size\n",
      "\u001b[0;31mKeyError\u001b[0m: 'HIDDEN_SIZE'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# [GRU Model with Luong Attention]\n",
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.score = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        # decoder_hidden: (batch_size, hidden_size)\n",
    "        # encoder_outputs: (batch_size, seq_len, hidden_size)\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1)  # (batch_size, 1, hidden_size)\n",
    "        energy = torch.tanh(self.score(encoder_outputs) + decoder_hidden)  # (batch_size, seq_len, hidden_size)\n",
    "        attention_weights = F.softmax(energy, dim=1)  # (batch_size, seq_len, hidden_size)\n",
    "        context = torch.bmm(attention_weights.transpose(1, 2), encoder_outputs)  # (batch_size, hidden_size, 1)\n",
    "        return context.squeeze(2)  # (batch_size, hidden_size)\n",
    "\n",
    "\n",
    "class GRUWithLuongAttention(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_size=CFG['HIDDEN_SIZE'], output_size=CFG['PREDICT_SIZE']):\n",
    "        super(GRUWithLuongAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.attention = LuongAttention(hidden_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_size // 2, output_size)\n",
    "        )\n",
    "        self.actv = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Shape: (B, TRAIN_WINDOW_SIZE, HIDDEN_SIZE)\n",
    "        output, hidden = self.gru(x)\n",
    "        # Apply Luong Attention\n",
    "        context = self.attention(hidden[-1], output)  # Shape: (B, HIDDEN_SIZE)\n",
    "        context = context.unsqueeze(1)  # Add the seq_len dimension to context tensor\n",
    "\n",
    "        # Reshape hidden[-1] to (B, 1, HIDDEN_SIZE)\n",
    "        hidden_reshaped = hidden[-1].unsqueeze(1)  # Shape: (B, 1, HIDDEN_SIZE)\n",
    "\n",
    "        # Concatenate hidden[-1] and context along dimension 2\n",
    "        x = torch.cat((hidden_reshaped, context), dim=2)  # Shape: (B, 1, HIDDEN_SIZE * 2)\n",
    "\n",
    "        # Fully connected layer\n",
    "        x = self.actv(self.fc(x.squeeze(1)))  # Reshape to (B, HIDDEN_SIZE * 2)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# [Model Training]\n",
    "\n",
    "def train_gru_with_attention(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    scaler = GradScaler()  # Initialize the GradScaler for mixed-precision training\n",
    "    best_loss = 9999999\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(1, CFG['EPOCHS'] + 1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for X, Y in tqdm(iter(train_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():  # Use mixed-precision training context\n",
    "                output = model(X)\n",
    "                loss = criterion(output, Y)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        val_loss = validation_gru_with_attention(model, val_loader, criterion, device)\n",
    "        print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : [{val_loss:.5f}]')\n",
    "\n",
    "        if best_loss > val_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "            print('Model Saved')\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def validation_gru_with_attention(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, Y in tqdm(iter(val_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(val_loss)\n",
    "\n",
    "# [Run !!]\n",
    "gru_with_attention_model = GRUWithLuongAttention()\n",
    "optimizer = torch.optim.Adam(params=gru_with_attention_model.parameters(), lr=CFG[\"LEARNING_RATE\"])\n",
    "trained_gru_with_attention_model = train_gru_with_attention(gru_with_attention_model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "# [Model Inference]\n",
    "test_dataset = CustomDataset(test_input, None)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X in tqdm(iter(test_loader)):\n",
    "            X = X.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "\n",
    "            # Move model output to CPU and convert to numpy array\n",
    "            output = output.cpu().numpy()\n",
    "\n",
    "            predictions.extend(output)\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "gru_with_attention_pred = inference(trained_gru_with_attention_model, test_loader, device)\n",
    "\n",
    "# [Inverse Scaling and Post-processing]\n",
    "for idx in range(len(gru_with_attention_pred)):\n",
    "    gru_with_attention_pred[idx, :] = gru_with_attention_pred[idx, :] * (scale_max_dict[idx] - scale_min_dict[idx]) + scale_min_dict[idx]\n",
    "\n",
    "# Post-processing\n",
    "gru_with_attention_pred = np.round(gru_with_attention_pred, 0).astype(int)\n",
    "\n",
    "# [Submission]\n",
    "submit = pd.read_csv('data/sample_submission.csv')\n",
    "submit.iloc[:, 1:] = gru_with_attention_pred\n",
    "submit.to_csv('data/gru_with_attention_submit.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b6c6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1aed1151",
   "metadata": {},
   "source": [
    "# Stacked GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "851892bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T05:45:46.686094Z",
     "start_time": "2023-08-07T05:45:33.059648Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc54510e8f343cc9c779cd4e7186e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_data))):\n\u001b[1;32m     33\u001b[0m     maxi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(train_data\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m4\u001b[39m:])\n\u001b[0;32m---> 34\u001b[0m     mini \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m maxi \u001b[38;5;241m==\u001b[39m mini:\n\u001b[1;32m     37\u001b[0m         train_data\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m4\u001b[39m:] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/data/program_files/anaconda3/lib/python3.10/site-packages/pandas/core/indexing.py:1067\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m   1066\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[0;32m-> 1067\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1069\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/data/program_files/anaconda3/lib/python3.10/site-packages/pandas/core/indexing.py:1565\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1563\u001b[0m tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tuple_indexer(tup)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[0;32m-> 1565\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_lowerdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple_same_dim(tup)\n",
      "File \u001b[0;32m/data/program_files/anaconda3/lib/python3.10/site-packages/pandas/core/indexing.py:967\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tup):\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_label_like(key):\n\u001b[1;32m    965\u001b[0m         \u001b[38;5;66;03m# We don't need to check for tuples here because those are\u001b[39;00m\n\u001b[1;32m    966\u001b[0m         \u001b[38;5;66;03m#  caught by the _is_nested_tuple_indexer check above.\u001b[39;00m\n\u001b[0;32m--> 967\u001b[0m         section \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m         \u001b[38;5;66;03m# We should never have a scalar section here, because\u001b[39;00m\n\u001b[1;32m    970\u001b[0m         \u001b[38;5;66;03m#  _getitem_lowerdim is only called after a check for\u001b[39;00m\n\u001b[1;32m    971\u001b[0m         \u001b[38;5;66;03m#  is_scalar_access, which that would be.\u001b[39;00m\n\u001b[1;32m    972\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m section\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim:\n\u001b[1;32m    973\u001b[0m             \u001b[38;5;66;03m# we're in the middle of slicing through a MultiIndex\u001b[39;00m\n\u001b[1;32m    974\u001b[0m             \u001b[38;5;66;03m# revise the key wrt to `section` by inserting an _NS\u001b[39;00m\n",
      "File \u001b[0;32m/data/program_files/anaconda3/lib/python3.10/site-packages/pandas/core/indexing.py:1627\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1624\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[1;32m   1625\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_integer(key, axis)\n\u001b[0;32m-> 1627\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ixs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/program_files/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:3720\u001b[0m, in \u001b[0;36mDataFrame._ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   3718\u001b[0m \u001b[38;5;66;03m# if we are a copy, mark as such\u001b[39;00m\n\u001b[1;32m   3719\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(new_mgr\u001b[38;5;241m.\u001b[39marray, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m new_mgr\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mbase \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3720\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor_sliced\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_mgr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[1;32m   3721\u001b[0m     \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   3722\u001b[0m )\n\u001b[1;32m   3723\u001b[0m result\u001b[38;5;241m.\u001b[39m_set_is_copy(\u001b[38;5;28mself\u001b[39m, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m   3724\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/data/program_files/anaconda3/lib/python3.10/site-packages/pandas/core/series.py:342\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    337\u001b[0m rdiv: Callable[[Series, Any], Series]\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# ----------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# Constructors\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    344\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    345\u001b[0m     index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    346\u001b[0m     dtype: Dtype \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    347\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    348\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    349\u001b[0m     fastpath: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    350\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(data, (SingleBlockManager, SingleArrayManager))\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     ):\n\u001b[1;32m    358\u001b[0m         \u001b[38;5;66;03m# GH#33357 called with just the SingleBlockManager\u001b[39;00m\n\u001b[1;32m    359\u001b[0m         NDFrame\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# [Hyperparameter Setting]\n",
    "CFG = {\n",
    "    'TRAIN_WINDOW_SIZE': 90,  # 90일치로 학습\n",
    "    'PREDICT_SIZE': 21,  # 21일치 예측\n",
    "    'EPOCHS': 10,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'BATCH_SIZE': 1024,\n",
    "    'SEED': 41,\n",
    "    'STACKED_GRU_LAYERS': 2  # Number of stacked GRU layers\n",
    "}\n",
    "\n",
    "# [Import data]\n",
    "train_data = pd.read_csv('data/train.csv').drop(columns=['ID', '제품'])\n",
    "\n",
    "# [Data Preprocessing]\n",
    "scale_max_dict = {}\n",
    "scale_min_dict = {}\n",
    "\n",
    "for idx in tqdm(range(len(train_data))):\n",
    "    maxi = np.max(train_data.iloc[idx, 4:])\n",
    "    mini = np.min(train_data.iloc[idx, 4:])\n",
    "\n",
    "    if maxi == mini:\n",
    "        train_data.iloc[idx, 4:] = 0\n",
    "    else:\n",
    "        train_data.iloc[idx, 4:] = (train_data.iloc[idx, 4:] - mini) / (maxi - mini)\n",
    "\n",
    "    scale_max_dict[idx] = maxi\n",
    "    scale_min_dict[idx] = mini\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = ['대분류', '중분류', '소분류', '브랜드']\n",
    "\n",
    "\n",
    "for col in categorical_columns:\n",
    "    label_encoder.fit(train_data[col])\n",
    "    train_data[col] = label_encoder.transform(train_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7e39e36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T14:27:09.223426Z",
     "start_time": "2023-08-05T14:25:39.361493Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d07d5ff8114875959a8af60d65f249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b12e42e08b41fea976b97864b9e020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def make_train_data(data, train_size=CFG['TRAIN_WINDOW_SIZE'], predict_size=CFG['PREDICT_SIZE']):\n",
    "    num_rows = len(data)\n",
    "    window_size = train_size + predict_size\n",
    "\n",
    "    input_data = np.empty((num_rows * (len(data.columns) - window_size + 1), train_size, len(data.iloc[0, :4]) + 1))\n",
    "    target_data = np.empty((num_rows * (len(data.columns) - window_size + 1), predict_size))\n",
    "\n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :4])\n",
    "        sales_data = np.array(data.iloc[i, 4:])\n",
    "\n",
    "        for j in range(len(sales_data) - window_size + 1):\n",
    "            window = sales_data[j: j + window_size]\n",
    "            temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))\n",
    "            input_data[i * (len(data.columns) - window_size + 1) + j] = temp_data\n",
    "            target_data[i * (len(data.columns) - window_size + 1) + j] = window[train_size:]\n",
    "\n",
    "    return input_data, target_data\n",
    "\n",
    "def make_predict_data(data, train_size=CFG['TRAIN_WINDOW_SIZE']):\n",
    "    num_rows = len(data)\n",
    "    input_data = np.empty((num_rows, train_size, len(data.iloc[0, :4]) + 1))\n",
    "\n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :4])\n",
    "        sales_data = np.array(data.iloc[i, -train_size:])\n",
    "\n",
    "        window = sales_data[-train_size:]\n",
    "        temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))\n",
    "        input_data[i] = temp_data\n",
    "\n",
    "    return input_data\n",
    "\n",
    "train_input, train_target = make_train_data(train_data)\n",
    "test_input = make_predict_data(train_data)\n",
    "\n",
    "# Train / Validation Split\n",
    "data_len = len(train_input)\n",
    "val_input = train_input[-int(data_len * 0.2):]\n",
    "val_target = train_target[-int(data_len * 0.2):]\n",
    "train_input = train_input[:-int(data_len * 0.2)]\n",
    "train_target = train_target[:-int(data_len * 0.2)]\n",
    "\n",
    "# [Custom Dataset]\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.Y is not None:\n",
    "            return torch.Tensor(self.X[index]), torch.Tensor(self.Y[index])\n",
    "        return torch.Tensor(self.X[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "train_dataset = CustomDataset(train_input, train_target)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = CustomDataset(val_input, val_target)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0e02f8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T14:27:11.001843Z",
     "start_time": "2023-08-05T14:27:09.225286Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN version incompatibility: PyTorch was compiled  against (8, 5, 0) but found runtime version (8, 1, 0). PyTorch already comes bundled with cuDNN. One option to resolving this error is to ensure PyTorch can find the bundled cuDNN.Looks like your LD_LIBRARY_PATH contains incompatible version of cudnnPlease either remove it from the path or install cudnn (8, 5, 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 89\u001b[0m\n\u001b[1;32m     87\u001b[0m stacked_gru_model \u001b[38;5;241m=\u001b[39m StackedGRUModel()\n\u001b[1;32m     88\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(params\u001b[38;5;241m=\u001b[39mstacked_gru_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mCFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLEARNING_RATE\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 89\u001b[0m trained_stacked_gru_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_stacked_gru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstacked_gru_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# [Model Inference]\u001b[39;00m\n\u001b[1;32m     92\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(test_input, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[3], line 37\u001b[0m, in \u001b[0;36mtrain_stacked_gru\u001b[0;34m(model, optimizer, train_loader, val_loader, device)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_stacked_gru\u001b[39m(model, optimizer, train_loader, val_loader, device):\n\u001b[0;32m---> 37\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     39\u001b[0m     best_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9999999\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:202\u001b[0m, in \u001b[0;36mRNNBase._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    197\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Resets _flat_weights\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# Note: be v. careful before removing this, as 3rd party device types\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# likely rely on this behavior to properly .to() modules like LSTM.\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_flat_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:139\u001b[0m, in \u001b[0;36mRNNBase._init_flat_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, wn) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, wn) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    136\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m wn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights_names]\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weight_refs \u001b[38;5;241m=\u001b[39m [weakref\u001b[38;5;241m.\u001b[39mref(w) \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    138\u001b[0m                           \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights]\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:169\u001b[0m, in \u001b[0;36mRNNBase.flatten_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m dtype \u001b[38;5;241m=\u001b[39m first_fw\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fw \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights:\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fw\u001b[38;5;241m.\u001b[39mdata, Tensor) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (fw\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m dtype) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    168\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m fw\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mis_cuda \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_acceptable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# If any parameters alias, we fall back to the slower, copying code path. This is\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# a sufficient check, because overlapping parameter buffers that don't completely\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# alias would break the assumptions of the uniqueness check in\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Module.named_parameters().\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/backends/cudnn/__init__.py:97\u001b[0m, in \u001b[0;36mis_acceptable\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m     93\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyTorch was compiled without cuDNN/MIOpen support. To use cuDNN/MIOpen, rebuild \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyTorch making sure the library is visible to the build system.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     98\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuDNN/MIOpen library not found. Check your \u001b[39m\u001b[38;5;132;01m{libpath}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     99\u001b[0m         libpath\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    100\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdarwin\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDYLD_LIBRARY_PATH\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    101\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPATH\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    102\u001b[0m         }\u001b[38;5;241m.\u001b[39mget(sys\u001b[38;5;241m.\u001b[39mplatform, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLD_LIBRARY_PATH\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/backends/cudnn/__init__.py:52\u001b[0m, in \u001b[0;36m_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m ld_library_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLD_LIBRARY_PATH\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(substring \u001b[38;5;129;01min\u001b[39;00m ld_library_path \u001b[38;5;28;01mfor\u001b[39;00m substring \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcudnn\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_error_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     53\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLooks like your LD_LIBRARY_PATH contains incompatible version of cudnn\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     54\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease either remove it from the path or install cudnn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompile_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_error_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     57\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mone possibility is that there is a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     58\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconflicting cuDNN in LD_LIBRARY_PATH.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN version incompatibility: PyTorch was compiled  against (8, 5, 0) but found runtime version (8, 1, 0). PyTorch already comes bundled with cuDNN. One option to resolving this error is to ensure PyTorch can find the bundled cuDNN.Looks like your LD_LIBRARY_PATH contains incompatible version of cudnnPlease either remove it from the path or install cudnn (8, 5, 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# [Stacked GRU Model]\n",
    "class StackedGRUModel(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_size=512, output_size=CFG['PREDICT_SIZE'], num_layers=CFG['STACKED_GRU_LAYERS']):\n",
    "        super(StackedGRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_size // 2, output_size)\n",
    "        )\n",
    "        self.actv = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size, x.device)\n",
    "\n",
    "        # GRU layer\n",
    "        gru_out, hidden = self.gru(x, hidden)\n",
    "\n",
    "        # Only use the last output sequence\n",
    "        last_output = gru_out[:, -1, :]\n",
    "\n",
    "        # Fully connected layer\n",
    "        output = self.actv(self.fc(last_output))\n",
    "\n",
    "        return output.squeeze(1)\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # Initialize hidden state\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "# [Model Training]\n",
    "def train_stacked_gru(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    best_loss = 9999999\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(1, CFG['EPOCHS'] + 1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "\n",
    "        for X, Y in tqdm(iter(train_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        val_loss = validation_stacked_gru(model, val_loader, criterion, device)\n",
    "        print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : [{val_loss:.5f}]')\n",
    "\n",
    "        if best_loss > val_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "            print('Model Saved')\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def validation_stacked_gru(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, Y in tqdm(iter(val_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(val_loss)\n",
    "\n",
    "# [Run !!]\n",
    "stacked_gru_model = StackedGRUModel()\n",
    "optimizer = torch.optim.Adam(params=stacked_gru_model.parameters(), lr=CFG[\"LEARNING_RATE\"])\n",
    "trained_stacked_gru_model = train_stacked_gru(stacked_gru_model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "# [Model Inference]\n",
    "test_dataset = CustomDataset(test_input, None)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X in tqdm(iter(test_loader)):\n",
    "            X = X.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "\n",
    "            # Move model output to CPU and convert to numpy array\n",
    "            output = output.cpu().numpy()\n",
    "\n",
    "            predictions.extend(output)\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "stacked_gru_pred = inference(trained_stacked_gru_model, test_loader, device)\n",
    "\n",
    "# [Inverse Scaling and Post-processing]\n",
    "for idx in range(len(stacked_gru_pred)):\n",
    "    stacked_gru_pred[idx, :] = stacked_gru_pred[idx, :] * (scale_max_dict[idx] - scale_min_dict[idx]) + scale_min_dict[idx]\n",
    "\n",
    "# Post-processing\n",
    "stacked_gru_pred = np.round(stacked_gru_pred, 0).astype(int)\n",
    "\n",
    "# [Submission]\n",
    "submit = pd.read_csv('data/sample_submission.csv')\n",
    "submit.iloc[:, 1:] = stacked_gru_pred\n",
    "submit.to_csv('data/stacked_gru_submit.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e653b6b",
   "metadata": {},
   "source": [
    "# Stacked GRU with Luong Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab6f5278",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T08:09:44.066590Z",
     "start_time": "2023-08-07T08:09:42.875402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0562396f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T07:42:15.026513Z",
     "start_time": "2023-08-07T07:42:15.022069Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca7930aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T07:57:02.898366Z",
     "start_time": "2023-08-07T07:53:43.657500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "419ffcf34f2b41f79d253e24d0d34357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# [Hyperparameter Setting]\n",
    "CFG = {\n",
    "    'TRAIN_WINDOW_SIZE': 90,  # 90일치로 학습\n",
    "    'PREDICT_SIZE': 21,  # 21일치 예측\n",
    "    'EPOCHS': 10,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'BATCH_SIZE': 512,\n",
    "    'SEED': 41,\n",
    "    'HIDDEN_SIZE': 512,  # Hidden size of GRU model\n",
    "    'NUM_LAYERS': 2,  # Number of stacked GRU layers\n",
    "}\n",
    "\n",
    "# [Import data]\n",
    "train_data = pd.read_csv('data/train.csv').drop(columns=['ID', '제품'])\n",
    "\n",
    "# [Data Preprocessing]\n",
    "scale_max_dict = {}\n",
    "scale_min_dict = {}\n",
    "\n",
    "for idx in tqdm(range(len(train_data))):\n",
    "    maxi = np.max(train_data.iloc[idx, 4:])\n",
    "    mini = np.min(train_data.iloc[idx, 4:])\n",
    "\n",
    "    if maxi == mini:\n",
    "        train_data.iloc[idx, 4:] = 0\n",
    "    else:\n",
    "        train_data.iloc[idx, 4:] = (train_data.iloc[idx, 4:] - mini) / (maxi - mini)\n",
    "\n",
    "    scale_max_dict[idx] = maxi\n",
    "    scale_min_dict[idx] = mini\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = ['대분류', '중분류', '소분류', '브랜드']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    label_encoder.fit(train_data[col])\n",
    "    train_data[col] = label_encoder.transform(train_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3bf4125",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T07:58:29.535764Z",
     "start_time": "2023-08-07T07:57:02.900617Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7873b26914c346fb80044096a3560348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731350ada2f74a0db280eb5524c4160f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_train_data(data, train_size=CFG['TRAIN_WINDOW_SIZE'], predict_size=CFG['PREDICT_SIZE']):\n",
    "    num_rows = len(data)\n",
    "    window_size = train_size + predict_size\n",
    "\n",
    "    input_data = np.empty((num_rows * (len(data.columns) - window_size + 1), train_size, len(data.iloc[0, :4]) + 1))\n",
    "    target_data = np.empty((num_rows * (len(data.columns) - window_size + 1), predict_size))\n",
    "\n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :4])\n",
    "        sales_data = np.array(data.iloc[i, 4:])\n",
    "\n",
    "        for j in range(len(sales_data) - window_size + 1):\n",
    "            window = sales_data[j: j + window_size]\n",
    "            temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))\n",
    "            input_data[i * (len(data.columns) - window_size + 1) + j] = temp_data\n",
    "            target_data[i * (len(data.columns) - window_size + 1) + j] = window[train_size:]\n",
    "\n",
    "    return input_data, target_data\n",
    "\n",
    "def make_predict_data(data, train_size=CFG['TRAIN_WINDOW_SIZE']):\n",
    "    num_rows = len(data)\n",
    "    input_data = np.empty((num_rows, train_size, len(data.iloc[0, :4]) + 1))\n",
    "\n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :4])\n",
    "        sales_data = np.array(data.iloc[i, -train_size:])\n",
    "\n",
    "        window = sales_data[-train_size:]\n",
    "        temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))\n",
    "        input_data[i] = temp_data\n",
    "\n",
    "    return input_data\n",
    "\n",
    "train_input, train_target = make_train_data(train_data)\n",
    "test_input = make_predict_data(train_data)\n",
    "\n",
    "# Train / Validation Split\n",
    "data_len = len(train_input)\n",
    "val_input = train_input[-int(data_len * 0.2):]\n",
    "val_target = train_target[-int(data_len * 0.2):]\n",
    "train_input = train_input[:-int(data_len * 0.2)]\n",
    "train_target = train_target[:-int(data_len * 0.2)]\n",
    "\n",
    "# [Custom Dataset]\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.Y is not None:\n",
    "            return torch.Tensor(self.X[index]), torch.Tensor(self.Y[index])\n",
    "        return torch.Tensor(self.X[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "train_dataset = CustomDataset(train_input, train_target)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = CustomDataset(val_input, val_target)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "095d7417",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T07:58:30.094591Z",
     "start_time": "2023-08-07T07:58:29.537956Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8238a6e2fdf4b1e846daa6e0ffc0fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8765 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 23.65 GiB total capacity; 327.26 MiB already allocated; 67.81 MiB free; 350.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 95\u001b[0m\n\u001b[1;32m     93\u001b[0m stacked_gru_with_attention_model \u001b[38;5;241m=\u001b[39m StackedGRUWithLuongAttention()\n\u001b[1;32m     94\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(params\u001b[38;5;241m=\u001b[39mstacked_gru_with_attention_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mCFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLEARNING_RATE\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 95\u001b[0m trained_stacked_gru_with_attention_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_stacked_gru_with_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstacked_gru_with_attention_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# [Model Inference]\u001b[39;00m\n\u001b[1;32m     98\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(test_input, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[10], line 58\u001b[0m, in \u001b[0;36mtrain_stacked_gru_with_attention\u001b[0;34m(model, optimizer, train_loader, val_loader, device)\u001b[0m\n\u001b[1;32m     54\u001b[0m Y \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     56\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 58\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, Y)\n\u001b[1;32m     61\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[10], line 36\u001b[0m, in \u001b[0;36mStackedGRUWithLuongAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     35\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)  \u001b[38;5;66;03m# Shape: (B, TRAIN_WINDOW_SIZE, HIDDEN_SIZE)\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(hidden, output)\n\u001b[1;32m     38\u001b[0m     output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((context, hidden[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:998\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    997\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 998\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1001\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   1002\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 23.65 GiB total capacity; 327.26 MiB already allocated; 67.81 MiB free; 350.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# [GRU Model with Luong Attention]\n",
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.score = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        # decoder_hidden: (num_layers * num_directions, batch_size, hidden_size)\n",
    "        # encoder_outputs: (batch_size, seq_len, hidden_size)\n",
    "        decoder_hidden = decoder_hidden[-1]  # Select the last layer (num_layers * num_directions)\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1)  # (batch_size, 1, hidden_size)\n",
    "        energy = torch.tanh(self.score(encoder_outputs) + decoder_hidden)  # (batch_size, seq_len, hidden_size)\n",
    "        attention_weights = F.softmax(energy, dim=1)  # (batch_size, seq_len, hidden_size)\n",
    "        context = torch.bmm(attention_weights.transpose(1, 2), encoder_outputs)  # (batch_size, 1, hidden_size)\n",
    "        return context.squeeze(1)  # (batch_size, hidden_size)\n",
    "\n",
    "class StackedGRUWithLuongAttention(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_size=CFG['HIDDEN_SIZE'], output_size=CFG['PREDICT_SIZE'], num_layers=CFG['NUM_LAYERS']):\n",
    "        super(StackedGRUWithLuongAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.attention = LuongAttention(hidden_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_size // 2, output_size)\n",
    "        )\n",
    "        self.actv = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Shape: (B, TRAIN_WINDOW_SIZE, HIDDEN_SIZE)\n",
    "        output, hidden = self.gru(x)\n",
    "        context = self.attention(hidden, output)\n",
    "        output = torch.cat((context, hidden[-1]), dim=1)\n",
    "        output = self.actv(self.fc(output))\n",
    "        return output.squeeze(1)\n",
    "\n",
    "# [Model Training]\n",
    "def train_stacked_gru_with_attention(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    best_loss = 9999999\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(1, CFG['EPOCHS'] + 1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for X, Y in tqdm(iter(train_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        val_loss = validation_stacked_gru_with_attention(model, val_loader, criterion, device)\n",
    "        print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : [{val_loss:.5f}]')\n",
    "\n",
    "        if best_loss > val_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "            print('Model Saved')\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def validation_stacked_gru_with_attention(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, Y in tqdm(iter(val_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(val_loss)\n",
    "\n",
    "# [Run !!]\n",
    "stacked_gru_with_attention_model = StackedGRUWithLuongAttention()\n",
    "optimizer = torch.optim.Adam(params=stacked_gru_with_attention_model.parameters(), lr=CFG[\"LEARNING_RATE\"])\n",
    "trained_stacked_gru_with_attention_model = train_stacked_gru_with_attention(stacked_gru_with_attention_model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "# [Model Inference]\n",
    "test_dataset = CustomDataset(test_input, None)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X in tqdm(iter(test_loader)):\n",
    "            X = X.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "\n",
    "            # Move model output to CPU and convert to numpy array\n",
    "            output = output.cpu().numpy()\n",
    "\n",
    "            predictions.extend(output)\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "stacked_gru_with_attention_pred = inference(trained_stacked_gru_with_attention_model, test_loader, device)\n",
    "\n",
    "# [Inverse Scaling and Post-processing]\n",
    "for idx in range(len(stacked_gru_with_attention_pred)):\n",
    "    stacked_gru_with_attention_pred[idx, :] = stacked_gru_with_attention_pred[idx, :] * (scale_max_dict[idx] - scale_min_dict[idx]) + scale_min_dict[idx]\n",
    "\n",
    "# Post-processing\n",
    "stacked_gru_with_attention_pred = np.round(stacked_gru_with_attention_pred, 0).astype(int)\n",
    "\n",
    "# [Submission]\n",
    "submit = pd.read_csv('data/sample_submission.csv')\n",
    "submit.iloc[:, 1:] = stacked_gru_with_attention_pred\n",
    "submit.to_csv('data/stacked_gru_with_attention_submit.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9017417f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T07:49:20.101177Z",
     "start_time": "2023-08-07T07:49:19.904580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/cuda/lib64:/usr/local/cuda-11.2/lib64\r\n"
     ]
    }
   ],
   "source": [
    "!echo $LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9019e187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
